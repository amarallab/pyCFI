{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pyCFI Development Notebook - Improved Full 3D Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "- Open issues are marked in comments using the word `FLAG`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports\n",
    "\n",
    "import itertools, collections\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "from skimage import io\n",
    "from scipy import spatial\n",
    "from scipy import interpolate\n",
    "from scipy import optimize\n",
    "import sympy as sym\n",
    "\n",
    "from ipywidgets import interact\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parameters & Settings\n",
    "\n",
    "fpath = '../Data/Generated/three_intersecting_spheres_aniso.tif'\n",
    "res   = np.array([1.0, 0.5, 0.5])  # Voxel sizes (z,y,x) in microns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load input segmentation stack\n",
    "\n",
    "im = io.imread(fpath)\n",
    "print(im.dtype, im.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Show input stack\n",
    "\n",
    "@interact(z=(0, im.shape[0]-1, 1))\n",
    "def show_stack(z=im.shape[0]//2):\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.imshow(im[z], cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying Object Outlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Identify outline voxels by comparing shifted images\n",
    "\n",
    "# Pad the image by 1 voxel on all sides\n",
    "im_pad = np.pad(im, 1, mode='reflect')\n",
    "\n",
    "# Get possible shifts in all directions\n",
    "shifts = itertools.product([0,1], repeat=3)\n",
    "\n",
    "# Check and accumulate differences in shifts\n",
    "outlines = np.zeros_like(im, dtype=np.bool)\n",
    "for shift in shifts:\n",
    "    zs0, ys0, xs0 = [slice(1, None) if s else slice(None) for s in shift]\n",
    "    zs1, ys1, xs1 = [slice(None,-1) if s else slice(None) for s in shift]\n",
    "    comparison = im_pad[zs0, ys0, xs0] != im_pad[zs1, ys1, xs1]\n",
    "    outlines  += comparison[:im.shape[0],  :im.shape[1],  :im.shape[2]]\n",
    "    outlines  += comparison[-im.shape[0]:, -im.shape[1]:, -im.shape[2]:]  # Symmetry\n",
    "    \n",
    "# Re-annotate the cell identities\n",
    "outlines_id = outlines * im\n",
    "\n",
    "# Report\n",
    "print(outlines.dtype, outlines.shape)\n",
    "print(outlines_id.dtype, outlines_id.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Show identified outlines\n",
    "\n",
    "@interact(z=(0, im.shape[0]-1, 1))\n",
    "def show_stack(z=im.shape[0]//2):\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.imshow(outlines_id[z], cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying Triple Nodes (TNs) and Triple Junctions (TJs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Find coordinates of all voxels involved in triple junctions\n",
    "\n",
    "# FLAG: PERFORMANCE -- This could potentially be done with image shifting much like `outlines` above!\n",
    "# FLAG: PRECISION -- For the coordinates, would a `+1.0` be more appropriate at interfaces between two cells?\n",
    "\n",
    "# Get Outline Indices (OIs) and Outline Coordinates (OCs)\n",
    "OIs = np.array(np.where(outlines)).T\n",
    "OCs = (OIs + 0.5) * res\n",
    "\n",
    "# Go through OIs and find TN Indices (TNIs)\n",
    "TNIs = []\n",
    "for OI in OIs:\n",
    "    selection = im_pad[OI[0]+1:OI[0]+3, OI[1]+1:OI[1]+3, OI[2]+1:OI[2]+3]\n",
    "    if len(set(selection.flatten())) == 3:\n",
    "        TNIs.append(OI)\n",
    "TNIs = np.array(TNIs)\n",
    "\n",
    "# Convert to TN Coordinates (TNCs)\n",
    "TNCs = (TNIs + 0.5) * res\n",
    "\n",
    "# Report\n",
    "print('OCs: ', OCs.shape)\n",
    "print('TNIs:', TNIs.shape)\n",
    "print('TNCs:', TNCs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Build a dict of TJs structured as: {tuple(cell1_ID, cell2_ID, cell3_ID) : array(INDICES INTO TNIs/TNCs)}\n",
    "\n",
    "# Prepare defaultdict\n",
    "TJs = collections.defaultdict(lambda : [])\n",
    "\n",
    "# Go through TNs, create IDs, assign coordinates to IDs\n",
    "for idx,TNI in enumerate(TNIs):\n",
    "    selection = im_pad[np.int(TNI[0])+1:np.int(TNI[0])+3, \n",
    "                       np.int(TNI[1])+1:np.int(TNI[1])+3, \n",
    "                       np.int(TNI[2])+1:np.int(TNI[2])+3]\n",
    "    TJ_ID = tuple(sorted(set(selection.flatten())))\n",
    "    TJs[TJ_ID].append(idx)\n",
    "\n",
    "# Convert TJ lists to numpy arrays\n",
    "for key in TJs.keys():\n",
    "    TJs[key] = np.array(TJs[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Show identified TJs on image stack\n",
    "\n",
    "@interact(z=(0, im.shape[0]-1, 1))\n",
    "def show_stack(z=im.shape[0]//2):\n",
    "    \n",
    "    # Prep and plot image\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.imshow(outlines_id[z], cmap='gray')\n",
    "    \n",
    "    # For each TJ...\n",
    "    for TJ_num,TJ_ID in enumerate(TJs.keys()):\n",
    "        \n",
    "        # Get the TJ's TNs in the selected z plane\n",
    "        TNs_in_plane = TNIs[TJs[TJ_ID]][TNIs[TJs[TJ_ID]][:,0]==z]\n",
    "        \n",
    "        # Plot the points\n",
    "        plt.scatter(TNs_in_plane[:, 2], TNs_in_plane[:, 1],\n",
    "                    c=[TJ_num for _ in range(TNs_in_plane.shape[0])], # Coloring trick!\n",
    "                    cmap='hsv', vmin=0, vmax=len(TJs), s=20)\n",
    "        \n",
    "    # Finish\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Show identified TJs as 3D scatter\n",
    "\n",
    "# Prepare the plot\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "ax  = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot each TJ in a different color\n",
    "for TJ_num,TJ_ID in enumerate(TJs.keys()):\n",
    "    ax.scatter(TNCs[TJs[TJ_ID]][:,2], TNCs[TJs[TJ_ID]][:,1], TNCs[TJs[TJ_ID]][:,0],\n",
    "               c=[TJ_num for _ in range(TJs[TJ_ID].shape[0])], \n",
    "               cmap='hsv', vmin=0, vmax=len(TJs), s=10)\n",
    "\n",
    "## Also show cell outlines [may take several seconds to render!]\n",
    "#ax.scatter([c[2] for c in OCs],\n",
    "#           [c[1] for c in OCs],\n",
    "#           [c[0] for c in OCs],\n",
    "#           c='gray', alpha=0.01, linewidth=0, s=5)\n",
    "\n",
    "## Axis limits\n",
    "#ax.set_xlim([0,200])\n",
    "#ax.set_ylim([0,200])\n",
    "#ax.set_zlim([0,200])\n",
    "\n",
    "# Finish\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying Double Nodes (DNs) and Double Junctions (DJs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Find coordinates of all voxels involved in DOUBLE junctions\n",
    "\n",
    "# FLAG: PERFORMANCE -- Same as for TNI/TNC extraction above!\n",
    "# FLAG: PRECISION -- Same as for TNI/TNC extraction above!\n",
    "\n",
    "# Go through OIs and find DN Indices (DNIs)\n",
    "DNIs = []\n",
    "for OI in OIs:\n",
    "    selection = im_pad[OI[0]+1:OI[0]+3, \n",
    "                       OI[1]+1:OI[1]+3, \n",
    "                       OI[2]+1:OI[2]+3]\n",
    "    if len(set(selection.flatten())) == 2:\n",
    "        DNIs.append(OI)\n",
    "DNIs = np.array(DNIs)\n",
    "\n",
    "# Convert to DN Coordinates (DNCs)\n",
    "DNCs = (DNIs + 0.5) * res\n",
    "\n",
    "# Report\n",
    "print('OCs: ', OCs.shape)\n",
    "print('DNIs:', DNIs.shape)\n",
    "print('DNCs:', DNCs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Build a dict of Double Junctions (DJs) structured as: {tuple(cell1_ID, cell2_ID) : array(INDICES INTO DNIs/DNCs)}\n",
    "\n",
    "# Prepare defaultdict\n",
    "DJs = collections.defaultdict(lambda : [])\n",
    "\n",
    "# Go through DNs, create IDs, assign coordinates to IDs\n",
    "for idx,DNI in enumerate(DNIs):\n",
    "    selection = im_pad[np.int(DNI[0])+1:np.int(DNI[0])+3, \n",
    "                       np.int(DNI[1])+1:np.int(DNI[1])+3, \n",
    "                       np.int(DNI[2])+1:np.int(DNI[2])+3]\n",
    "    DJ_ID = tuple(sorted(set(selection.flatten())))\n",
    "    DJs[DJ_ID].append(idx)\n",
    "\n",
    "# Convert DJ lists to numpy arrays\n",
    "for key in DJs.keys():\n",
    "    DJs[key] = np.array(DJs[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Show identified DJs on image stack\n",
    "\n",
    "@interact(z=(0, im.shape[0]-1, 1))\n",
    "def show_stack(z=im.shape[0]//2):\n",
    "    \n",
    "    # Prep and plot image\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.imshow(outlines_id[z], cmap='gray')\n",
    "    \n",
    "    # For each DJ...\n",
    "    for DJ_num,DJ_ID in enumerate(DJs.keys()):\n",
    "        \n",
    "        # Get the DJ's DNs in the selected z plane\n",
    "        DNs_in_plane = DNIs[DJs[DJ_ID]][DNIs[DJs[DJ_ID]][:,0]==z]\n",
    "        \n",
    "        # Plot the points\n",
    "        plt.scatter(DNs_in_plane[:, 2], DNs_in_plane[:, 1],\n",
    "                    c=[DJ_num for _ in range(DNs_in_plane.shape[0])], # Coloring trick!\n",
    "                    cmap='hsv', vmin=0, vmax=len(DJs), s=5, lw=0, alpha=0.5)\n",
    "        \n",
    "    # Finish\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting Splines to TJs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns out spline fitting requires the input points to be roughly in order along the spline, which isn't guaranteed in our case. Ordering the points happens to be far harder problem than one might imagine (it's a variation of traveling salesman) but luckily it can be solved quite well with a Breadth-First Search (BFS). This solution is partially inspired by Imanol Luengo's answer to [this SO question](https://stackoverflow.com/questions/37742358/sorting-points-to-form-a-continuous-line).\n",
    "\n",
    "<font color=orange>**Warning 1:**</font> This will fail for geometries that exhibit \"crossings\" or \"forks\" of any kind. Although that should be very rare/non-existent in the data, a special form of \"fork\" is the circle. In case of a fully circular TJ, which occurs when two cells neatly touch each other, this will fail (unless some points are removed from the TJ). I couldn't come up with a way of fixing this but divised the `InvalidPathError` to at least pick up on such cases. However, **it may be too stringent** as it is currently implemented!\n",
    "\n",
    "<font color=orange>**Warning 2:**</font> Simply rescaling the z axis a little bit already led to renewed problems with this approach, so I'm starting to seriously doubt its robustness. We'll have to keep a close eye on this and possibly somehow develop a better solution if problems keep cropping up. Maybe some sort of modified graph search (rather than straight up BFS) would be a possibility..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function to reorder TEs along the progression of the TJ\n",
    "\n",
    "# FLAG: ROBUSTNESS -- I still have my doubts as to the robustness of this approach (see warnings above)!\n",
    "#                     I keep wondering if there isn't a better way!\n",
    "\n",
    "def sort_line_coords(coords, N_neighbors=10, source=None, \n",
    "                     return_argsort=False, ignore_path_check=False):\n",
    "    \"\"\"Given a set of coordinates that roughly lie on a 1D curve in mD space\n",
    "    (but may be in random order), sort the points such that they roughly follow \n",
    "    the curve's progression.\n",
    "    \n",
    "    Uses a breadth-first search tree on a nearest-neighbor graph of the coords,\n",
    "    which happens to result in the best possible sort. Does not work as intended\n",
    "    for closed curves and curves that form any kind of fork or crossing; an \n",
    "    Error is raised in such cases.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    coords : array of shape (N_points, M_dimensions)\n",
    "        Coordinates of points roughly lying on a point in M-dimensional space.\n",
    "    N_neighbors : int, optional, default 10\n",
    "        Number of nearest neighbors to include for each graph. If this is set\n",
    "        too low, connected components may form and no complete solution is\n",
    "        possible (raises an Exception). If this is set too high, the resulting\n",
    "        sort is very imprecises. The ideal value must be determined empirically.\n",
    "        When used to prepare TJs for spline fitting in the context of pyCFI, the\n",
    "        default (10) is a reasonably choice and the outcome is largely robust\n",
    "        to changes between values of 5 and 20.\n",
    "    source : None or int, optional, default None\n",
    "        The source is a point at one of the two ends of the line. If None, the\n",
    "        point is automatically determined by testing all different points and \n",
    "        selecting the one that yields the best sort (by minimizing the resulting\n",
    "        path distance). If source is an int, it indexes into coords to specify\n",
    "        the end point from which the sort is constructed. This saves a lot of\n",
    "        time compared to the automated search, especially if there are many\n",
    "        points, however it requires prior knowledge of the end point.\n",
    "    return_argsort : bool, optional, default False\n",
    "        If True, the index array that sorts the points into the best order is \n",
    "        returned as a second result. Otherwise, only a sorted version of coords \n",
    "        is returned.\n",
    "    ignore_path_check : bool, optional, default False\n",
    "        If True, the final path is not cross-checked and no InvalidPathErrors\n",
    "        can be raised (see Exceptions below).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    sorted_coords : array of shape (N_points, M_dimensions)\n",
    "        The same set of points as in the input coords but sorted along the\n",
    "        curve's progression in space.\n",
    "    best_path : array of shape (N_points,)\n",
    "\n",
    "Index array that sorts points along the curve's progression in space. \n",
    "        Only returned if return_argsort is set to True.\n",
    "        \n",
    "    Exceptions\n",
    "    ----------\n",
    "    InvalidPathError : If the curve is closed or contains forks/crossings, the\n",
    "        sort fails, which is reflected in the fact that the final path will\n",
    "        contain steps that do not have corresponding edges on the graph. In\n",
    "        this case, InvalidPathError is raised. This may also occur under other\n",
    "        dubious circumstances, e.g. if the input data is not a curve at all \n",
    "        or if it is a very broad curve or if N_neighbors is too low.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get pairwise distances (if needed)\n",
    "    if source is None:\n",
    "        dists = spatial.distance.squareform(spatial.distance.pdist(coords))\n",
    "    \n",
    "    # Get nearest neighbors\n",
    "    kdtree  = spatial.cKDTree(coords)\n",
    "    _, KNNs = kdtree.query(coords, k=N_neighbors)\n",
    "    \n",
    "    # Build adjacency matrix\n",
    "    adj_M = np.zeros((coords.shape[0], coords.shape[0]), dtype=np.bool)\n",
    "    for i,N in enumerate(KNNs):\n",
    "        adj_M[i,N] = True\n",
    "    \n",
    "    # Construct networkx graph\n",
    "    G = nx.from_numpy_array(adj_M)\n",
    "    if not nx.is_connected(G):\n",
    "        raise Exception('sort_line_coords: adjacency graph is not fully connected!')\n",
    "     \n",
    "    # If a source node is given, just get its BFS tree\n",
    "    if source is not None:\n",
    "        best_path = list(nx.bfs_tree(G, source))\n",
    "        \n",
    "    # Otherwise, find the best BFS tree from all sources\n",
    "    if source is None:\n",
    "        paths = []\n",
    "        costs = []\n",
    "        for n in G.nodes():\n",
    "\n",
    "            # Get BFS tree\n",
    "            path = list(nx.bfs_tree(G, n))\n",
    "\n",
    "            # Get sum of all distances within tree\n",
    "            cost = 0.0\n",
    "            for n0,n1 in zip(path, path[1:]):\n",
    "                cost += dists[n0, n1]\n",
    "\n",
    "            # Keep results\n",
    "            paths.append(path)\n",
    "            costs.append(cost)\n",
    "\n",
    "        # Select the best solution\n",
    "        best_path = paths[np.argmin(costs)]\n",
    "    \n",
    "    # Test for cases that probably failed\n",
    "    if not ignore_path_check:\n",
    "        for p1,p2 in zip(best_path, best_path[1:]):\n",
    "            if not G.has_edge(p1,p2):\n",
    "                class InvalidPathError(Exception): pass\n",
    "                raise InvalidPathError(\"The sort path uses an edge that is not on the graph. \"+\n",
    "                                       \"This should not happen and probably implies that the \"+\n",
    "                                       \"curve is cyclical or has a fork/crossing.\")\n",
    "    \n",
    "    # Sort coords and return\n",
    "    if return_argsort:\n",
    "        return coords[best_path], best_path\n",
    "    else:\n",
    "        return coords[best_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### A quick test of the TJ sorting\n",
    "\n",
    "# FLAG: ROBUSTNESS -- Currently, ingore_path_check has to be set to True for this to work\n",
    "#                     when z is properly rescaled (although the sort overall actually\n",
    "#                     doesn't look too bad).\n",
    "\n",
    "# Grab a TJ and compute the sort\n",
    "TJCs = TNCs[TJs[list(TJs.keys())[0]]]\n",
    "sorted_TJCs = sort_line_coords(TJCs, ignore_path_check=True)\n",
    "\n",
    "# Get sorted pairwise distances\n",
    "sorted_dists = spatial.distance.squareform(spatial.distance.pdist(sorted_TJCs))\n",
    "\n",
    "# Plot pairwise distances\n",
    "fig, ax = plt.subplots(1, 2, figsize=(13,6.5))\n",
    "ax[0].imshow(sorted_dists)\n",
    "ax[0].set_title(\"sorted distances\")\n",
    "ax[0].set_xlabel('TNs'); ax[0].set_ylabel('TNs')\n",
    "\n",
    "# Plot path/tree on scatter\n",
    "ax[1].scatter(TJCs[:,1], TJCs[:,0], s=20, alpha=0.5)\n",
    "ax[1].plot(sorted_TJCs[:,1], sorted_TJCs[:,0], c='r', alpha=0.75)\n",
    "ax[1].set_title(\"path scatter\")\n",
    "ax[1].set_xlabel('y'); ax[1].set_ylabel('z')\n",
    "\n",
    "# Done\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Wrapper for spline fitting\n",
    "\n",
    "def wrap_splprep(coords, k=3, verbose=False):\n",
    "    \"\"\"Fit an nD spline with scipy.interpolate.splprep.\n",
    "    \n",
    "    coords : array (points, dimensions) : input data\n",
    "    k=3 : integer : degrees of freedom\n",
    "    verbose=False : bool : wether to print all outputs\n",
    "    \n",
    "    returns -> tck : tuple (knots, coefficients, k) : \n",
    "               fit parameters as used by splev\n",
    "    \"\"\"\n",
    "    \n",
    "    # Fit the spline and unpack the (weirdly packaged) results\n",
    "    tcku, fp, ier, msg = interpolate.splprep(coords.T, k=k, full_output=True)\n",
    "    tck, u = tcku\n",
    "\n",
    "    # Report the results\n",
    "    if verbose:\n",
    "        print ('\\nt (knots, tck[0]):\\n' , tck[0])\n",
    "        print ('\\nc (coefficients, tck[1]):\\n' , tck[1])\n",
    "        print ('\\nk (degree, tck[2]):' , tck[2])\n",
    "        print ('\\nu (evaluation points):\\n', u)\n",
    "        print ('\\nfp (residual error):', fp)\n",
    "        print ('\\nier (error code; success is ier<=0):', ier)\n",
    "        print ('\\nmsg (message from FITPACK):\\n', msg)\n",
    "        \n",
    "    # Raise an error if FITPACK indicates failure\n",
    "    if ier > 0:\n",
    "        raise Exception('ier is >0, indicating that FITPACK failed somehow. '+\n",
    "                        'The message from FITPACK was:\\n'+msg)\n",
    "        \n",
    "    # Return the only result relevant to spline evaluation\n",
    "    return tck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Perform sorting and spline fitting on all TJs\n",
    "\n",
    "# Parameters\n",
    "num_ts = 100    # Determines the number of TNs that will be analyzed throughout the rest of the pipeline\n",
    "tng_dv = 10e-2  # FLAG -- PRECISION: Should this be smaller? FLAG -- ROBUSTNESS: Should this scale with res?\n",
    "\n",
    "# Output dicts\n",
    "TJs_spline_tck     = {}  # Fitted splines for each TJ\n",
    "TJs_spline_t       = {}  # Parameter (t) values for evaluation\n",
    "TJs_spline_ev      = {}  # Evaluated splines (at each t) for each TJ\n",
    "TJs_spline_tangent = {}  # Tangents to splines for each TJ\n",
    "\n",
    "# For each TJ...\n",
    "for TJ_ID in TJs.keys():\n",
    "    \n",
    "    # Sort coordinates along the line\n",
    "    sorted_TJCs, TJ_argsort = sort_line_coords(TNCs[TJs[TJ_ID]],\n",
    "                                               return_argsort=True,\n",
    "                                               ignore_path_check=True)\n",
    "    TJs[TJ_ID] = TJs[TJ_ID][TJ_argsort]\n",
    "    \n",
    "    # Perform spline fitting\n",
    "    tck = wrap_splprep(sorted_TJCs)\n",
    "    TJs_spline_tck[TJ_ID] = tck\n",
    "    \n",
    "    # Evaluate the spline in 1000 regular intervals\n",
    "    TJs_spline_t[TJ_ID] = np.linspace(0.0, 1.0, num_ts)\n",
    "    ev = interpolate.splev(TJs_spline_t[TJ_ID], tck)\n",
    "    ev = np.array(ev).T\n",
    "    TJs_spline_ev[TJ_ID] = ev\n",
    "    \n",
    "    # Also evaluate with slight deviation forward and backward\n",
    "    evD1 = np.array(interpolate.splev(TJs_spline_t[TJ_ID]+tng_dv, tck)).T\n",
    "    evD2 = np.array(interpolate.splev(TJs_spline_t[TJ_ID]-tng_dv, tck)).T\n",
    "    \n",
    "    # Approximate the tangent vector as the sum of the deviatory vectors\n",
    "    tangent_vec = ((evD1 - ev) + (ev - evD2)) / 2.0\n",
    "    TJs_spline_tangent[TJ_ID] = tangent_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualize the fitted splines and the tangent vectors as 3D scatter\n",
    "\n",
    "# Prepare the plot\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "ax  = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot each TJ spline\n",
    "for TJ_num,TJ_ID in enumerate(TJs.keys()):\n",
    "    ax.plot(TJs_spline_ev[TJ_ID][:,2], \n",
    "            TJs_spline_ev[TJ_ID][:,1], \n",
    "            TJs_spline_ev[TJ_ID][:,0],\n",
    "            lw=3)\n",
    "\n",
    "# Add the tangent vectors\n",
    "for TJ_num, TJ_ID in enumerate(TJs.keys()):\n",
    "    for splpt, tngvec in zip(TJs_spline_ev[TJ_ID][::5], TJs_spline_tangent[TJ_ID][::5]):\n",
    "        plt.plot([splpt[2], splpt[2]+tngvec[2]],\n",
    "                 [splpt[1], splpt[1]+tngvec[1]],\n",
    "                 [splpt[0], splpt[0]+tngvec[0]],\n",
    "                 'r-', alpha=0.5)\n",
    "            \n",
    "## Axis limits\n",
    "#ax.set_xlim([0,200])\n",
    "#ax.set_ylim([0,200])\n",
    "#ax.set_zlim([0,200])\n",
    "\n",
    "# Finish\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projecting DNs onto TJ-Orthogonal Dihedral Planes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sympy function to project close-by outline points onto a TN's TJ-orthogonal plane\n",
    "\n",
    "# FLAG: PERFORMANCE -- Save the resulting numpy func so that the symbolic solving doesn't need \n",
    "#                      to be rerun each time the code is executed! This is probably best done\n",
    "#                      by copying the function out into a .py file and importing it from there.\n",
    "#                      In the process, axis keywords could perhaps be added to handle vectorized\n",
    "#                      execution across many points/planes (see issue flag below).\n",
    "# FLAG: ROBUSTNESS -- Simply doing Gram-Schmidt as we currently do does not preserve the\n",
    "#                     uv-coordinate system within the plane across multiple TNs of a TJ. \n",
    "#                     Under certain circumstances (when values of the normal vector cross\n",
    "#                     zero), this can even lead to sudden 'flipping' of the orientation of\n",
    "#                     the plane. Currently, this is implicitly being \"fixed\" downstream \n",
    "#                     since the vector triplets are being aligned by rotation and flipping\n",
    "#                     prior to their reduction to a consensus triplet. However, it might be\n",
    "#                     more clean and robust to do something slightly more sophisticated than\n",
    "#                     classical Gram-Schmidt in order to enforce consistency.\n",
    "\n",
    "# Import sympy symbols\n",
    "from sympy.abc import q,r,s,  x,y,z  # (normal vector), (point to be projected)\n",
    "\n",
    "# Use Gram-Schmidt orthogonalization to create orthonormal vectors defining the in-plane\n",
    "# coordinate system given three arbitrary vectors, the first of which is the normal vector\n",
    "# of the plane. The other two (defining the in-plane directionalities) are arbitrarily \n",
    "# chosen such that they will never fall onto the normal vector or onto each other.\n",
    "orthonormals = sym.GramSchmidt([sym.Matrix([q,         r,         s]),  # Normal vec to plane -> first coordinate vec\n",
    "                                sym.Matrix([q, 2*(r+0.1), 3*(s+0.1)]),  # Arbitrary vec not on the normal vec\n",
    "                                sym.Matrix([2*(q+0.1), 3*(r+0.1), s])], # Arbitrary vec not on either other vec\n",
    "                                orthonormal=True)           # Normalize resulting orthogonal vectors\n",
    "\n",
    "# With the resulting orthonormals defining the new coordinate system, the projection\n",
    "# of points into it is just a straightforward dot product.\n",
    "projection_pt = sym.Matrix([x, y, z])\n",
    "proj_d = orthonormals[0].dot(projection_pt)  # Distance from plane\n",
    "proj_u = orthonormals[1].dot(projection_pt)  # Coordinate along first axis in plane\n",
    "proj_v = orthonormals[2].dot(projection_pt)  # Coordinate along second axis in plane\n",
    "\n",
    "# Lambdify\n",
    "lambda_dist = sym.utilities.lambdify((q,r,s,x,y,z), proj_d, modules='numpy')\n",
    "lambda_u    = sym.utilities.lambdify((q,r,s,x,y,z), proj_u, modules='numpy')\n",
    "lambda_v    = sym.utilities.lambdify((q,r,s,x,y,z), proj_v, modules='numpy')\n",
    "\n",
    "# Wrap into a function (sequential)\n",
    "def p2p_projection(normal_vec, pt_coords):\n",
    "        \n",
    "    # Unpack inputs\n",
    "    q,r,s = normal_vec[2], normal_vec[1], normal_vec[0]\n",
    "    x,y,z = pt_coords[:,2], pt_coords[:,1], pt_coords[:,0]\n",
    "    \n",
    "    # Run projection\n",
    "    dists = np.abs(lambda_dist(q,r,s,x,y,z))\n",
    "    p_u   = lambda_u(q,r,s,x,y,z)\n",
    "    p_v   = lambda_v(q,r,s,x,y,z)\n",
    "    \n",
    "    # Pack and return outputs\n",
    "    projected = np.array([p_u, p_v]).T\n",
    "    return projected, dists\n",
    "\n",
    "## Wrap into a function (vectorized)\n",
    "## FLAG -- ISSUE: This does not work as intended! It runs but does not yield the same results\n",
    "##                as the sequential version. There is likely an missing `axis=` kwarg in one \n",
    "##                of the numpy functions substituted by lambdify. This could perhaps be fixed\n",
    "##                by manual inspection of the projection function.\n",
    "#def p2p_projection_vectorized(normal_vec, pt_coords):\n",
    "#    \n",
    "#    # Unpack inputs\n",
    "#    q,r,s = normal_vec[..., 2, np.newaxis], normal_vec[..., 1, np.newaxis], normal_vec[..., 0, np.newaxis]\n",
    "#    x,y,z = pt_coords[..., 2], pt_coords[..., 1], pt_coords[..., 0]\n",
    "#    \n",
    "#    # Run projection\n",
    "#    dists = np.abs(lambda_dist(q,r,s,x,y,z))\n",
    "#    p_u   = lambda_u(q,r,s,x,y,z)\n",
    "#    p_v   = lambda_v(q,r,s,x,y,z)\n",
    "#    \n",
    "#    # Pack and return outputs\n",
    "#    projected = np.rollaxis(np.array([p_u, p_v]), 2)\n",
    "#    projected = np.rollaxis(projected, 2)\n",
    "#    return projected, dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Project relevant DNs onto the TJ-orthogonal plane\n",
    "\n",
    "# Params\n",
    "close_points_radius =  25.0\n",
    "dist_points_keep    = 100\n",
    "\n",
    "# Prep output dicts\n",
    "TJs_DNs_proj = {}\n",
    "TJs_DNs_dist = {}\n",
    "\n",
    "# For each TJ...\n",
    "for TJ_ID in TJs_spline_ev.keys():\n",
    "    \n",
    "    # Find the IDs of the three connected interfaces\n",
    "    DJ_IDs = list(itertools.combinations(TJ_ID, 2))\n",
    "    \n",
    "    # Skip edge cases with more than 3\n",
    "    if len(DJ_IDs) > 3:\n",
    "        continue\n",
    "        \n",
    "    # Get corresponding TJ-normal vectors\n",
    "    proj_tangents = TJs_spline_tangent[TJ_ID]\n",
    "    \n",
    "    # Prep output lists\n",
    "    TJs_DNs_proj[TJ_ID] = {DJ_ID:[] for DJ_ID in DJ_IDs}\n",
    "    TJs_DNs_dist[TJ_ID] = {DJ_ID:[] for DJ_ID in DJ_IDs}\n",
    "    \n",
    "    # For each TN of the current TJ...\n",
    "    for TN_idx, TN in enumerate(TJs_spline_ev[TJ_ID]): \n",
    "        \n",
    "        # For each connected interface...\n",
    "        for DJ_idx, DJ_ID in enumerate(DJ_IDs):\n",
    "            \n",
    "            # Get all the DJ points of that interface\n",
    "            current_DNCs = DNCs[DJs[DJ_ID]]\n",
    "            \n",
    "            # If there are none, skip this TN\n",
    "            if current_DNCs.size == 0:\n",
    "                print(\"Skipped case at TJ_ID=\"+str(TJ_ID) + \", TN_idx=\" +str(TN_idx) + \n",
    "                      \", DJ_ID=\"+str(DJ_ID)+\" ->> lacks interface points!\")\n",
    "                TJs_DNs_proj[TJ_ID][DJ_ID].append(np.empty(0))\n",
    "                continue\n",
    "                \n",
    "            # Get the DN points close to the TN\n",
    "            kdtree = spatial.cKDTree(current_DNCs)\n",
    "            KNNs   = kdtree.query_ball_point(TN, close_points_radius)\n",
    "            \n",
    "            # If there are none, skip this TN\n",
    "            if not KNNs:\n",
    "                print(\"Skipped case at TJ_ID=\"+str(TJ_ID) + \", TN_idx=\" +str(TN_idx) + \n",
    "                      \", DJ_ID=\"+str(DJ_ID)+\" ->> no close-by neighbors!\")\n",
    "                TJs_DNs_proj[TJ_ID][DJ_ID].append(np.empty(0))\n",
    "                continue\n",
    "            \n",
    "            # Move the points onto the origin\n",
    "            current_DNCs = current_DNCs[KNNs] - TJs_spline_ev[TJ_ID][TN_idx]\n",
    "            \n",
    "            # Nothing *should* go wrong here - but if it does, first look into\n",
    "            # the way the arbitrary vectors for Gram-Schmidt are generated!\n",
    "            with np.errstate(divide='raise', invalid='raise'):\n",
    "                    \n",
    "                # Project the TN points onto the dihedral plane\n",
    "                projs, dists = p2p_projection(proj_tangents[TN_idx], current_DNCs)\n",
    "            \n",
    "            # Threshold on the distances; keep at most the closest n points\n",
    "            psort = np.argsort(dists)[:dist_points_keep]\n",
    "            projs = projs[psort]\n",
    "            dists = dists[psort]\n",
    "            \n",
    "            # Keep the results\n",
    "            TJs_DNs_proj[TJ_ID][DJ_ID].append(projs)\n",
    "            TJs_DNs_dist[TJ_ID][DJ_ID].append(dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualize the projections\n",
    "\n",
    "@interact(TJ_ID=list(TJs_DNs_proj.keys()),\n",
    "          TN_idx=(0,num_ts-1,1))\n",
    "def plot_proj(TJ_ID=list(TJs_DNs_proj.keys())[0],\n",
    "              TN_idx=num_ts//2):\n",
    "    \n",
    "    # Prep plot\n",
    "    plt.figure(figsize=(6,6))\n",
    "    \n",
    "    # For each adjacent DJ...\n",
    "    for DJ_ID in itertools.combinations(TJ_ID, 2):\n",
    "        \n",
    "        # Plot the projected points\n",
    "        plt.scatter(TJs_DNs_proj[TJ_ID][DJ_ID][TN_idx][:,1],\n",
    "                    TJs_DNs_proj[TJ_ID][DJ_ID][TN_idx][:,0],\n",
    "                    c=TJs_DNs_dist[TJ_ID][DJ_ID][TN_idx],\n",
    "                    cmap='viridis', alpha=0.5, lw=0)\n",
    "    \n",
    "    # Finish\n",
    "    plt.xlabel('u')\n",
    "    plt.ylabel('v')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving Incident Vectors in the Dihedral Plane\n",
    "\n",
    "**Note:** The arc fitting approach taken here is based on the second approach described in [this scipy cookbook entry](https://scipy-cookbook.readthedocs.io/items/Least_Squares_Circle.html). It could probably be further improved by using the third approach, i.e. by explicitly specifying the Jacobian function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Functions for circular arc fitting\n",
    "\n",
    "# FLAG -- PERFORMANCE: The arc fitting approach used here could be sped up by explicitly\n",
    "#                      specifying a Jacobian function, see the markdown note above.\n",
    "\n",
    "# Compute coordinates from angle\n",
    "def circle(r, cx, cy, alpha):\n",
    "    x = r*np.cos(alpha) + cx\n",
    "    y = r*np.sin(alpha) + cy\n",
    "    return np.array([y,x])\n",
    "\n",
    "# Compute radius/radii given a center and a point/multiple points\n",
    "def radius(xc, yc, x, y):\n",
    "    return np.sqrt((x-xc)**2 + (y-yc)**2)\n",
    "\n",
    "# Loss: distance of data points from mean circle\n",
    "def circle_loss(c, x, y):\n",
    "    radii = radius(c[0], c[1], x, y)\n",
    "    return radii - radii.mean()\n",
    "\n",
    "# Subtraction of n1 and n2, wrapping around at minimum and maximum\n",
    "def wrap_sub(n1, n2, minimum=-np.pi, maximum=np.pi):\n",
    "    s = n1 - n2 \n",
    "    try:\n",
    "        s[s<=minimum] = maximum + (s[s<=minimum] - minimum)\n",
    "        s[s>=maximum] = minimum + (s[s>=maximum] - maximum)\n",
    "    except TypeError:\n",
    "        if s <= minimum: s = maximum + (s - minimum)\n",
    "        if s >= maximum: s = minimum + (s - maximum)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Find incident vectors for each TN based on circular arc fitting\n",
    "\n",
    "# FLAG -- ROBUSTNESS: There is still an edge case in this where perfectly straight\n",
    "#                     lines are fit with a completely wrong (very small) circle.\n",
    "#                     Right now, this is handled as a \"silly exception\" for the\n",
    "#                     synthetic test sample, where the middle line between cells is\n",
    "#                     perfectly straight. The hope is that this will never occur\n",
    "#                     in real data - but if it does, the curent handling will\n",
    "#                     almost certainly fail, as it presupposes that the line is\n",
    "#                     not only perfectly straight but also perfectly aligned with\n",
    "#                     one of the image axes.\n",
    "\n",
    "# Prep output dict\n",
    "TJs_vec_proj = {}\n",
    "\n",
    "# For each TJ...\n",
    "for TJ_ID in TJs_DNs_proj.keys():\n",
    "    \n",
    "    # Prepare an appropriate result array\n",
    "    TJs_vec_proj[TJ_ID] = np.empty((TJs_spline_ev[TJ_ID].shape[0], 3, 2))  # Num. of TNs, 3 vectors, 2 dimensions\n",
    "    \n",
    "    # For each adjacent DJ...\n",
    "    for DJ_idx, DJ_ID in enumerate(list(itertools.combinations(TJ_ID, 2))):\n",
    "        \n",
    "        # For each TN along the TJ...\n",
    "        for TN_idx in range(len(TJs_DNs_proj[TJ_ID][DJ_ID])):\n",
    "\n",
    "            # Prep data for fitting\n",
    "            x = TJs_DNs_proj[TJ_ID][DJ_ID][TN_idx][:,1]\n",
    "            y = TJs_DNs_proj[TJ_ID][DJ_ID][TN_idx][:,0]\n",
    "            \n",
    "            # Catch silly exception where all the data is in a line (may cause artifacts)\n",
    "            is_silly_exception = False\n",
    "            if np.allclose(x, x[0]) or np.allclose(y, y[0]):\n",
    "                is_silly_exception = True\n",
    "            \n",
    "            # Fit a circle to the data\n",
    "            center, ier = optimize.leastsq(circle_loss, [np.mean(x), np.mean(y)], args=(x, y))\n",
    "            cx, cy = center\n",
    "            r      = radius(cx, cy, x, y).mean()\n",
    "            \n",
    "            # Get angular position of the TN point (which is the origin in the projection)\n",
    "            TN_alpha = np.arctan2(0.0-cy, 0.0-cx)\n",
    "\n",
    "            # Get correct sign for tangent vector direction\n",
    "            DNs_alpha = wrap_sub(np.arctan2(y-cy, x-cx), TN_alpha)\n",
    "            sign = np.sign(np.mean(DNs_alpha))\n",
    "\n",
    "            # Get tangent vector based on TN angle and small shift\n",
    "            TN_proj = circle(r, cx, cy, TN_alpha)\n",
    "            shifted = circle(r, cx, cy, TN_alpha+10e-5)\n",
    "            tangent = shifted - TN_proj\n",
    "            tangent = tangent * sign\n",
    "\n",
    "            # Handle the silly exception where all the data is in a line\n",
    "            if is_silly_exception:\n",
    "                tangent = np.array([np.mean(y), np.mean(x)])\n",
    "            \n",
    "            # Normalize to magnitude 1\n",
    "            tangent = tangent / np.sqrt(np.sum(tangent**2.0))\n",
    "\n",
    "            # Save the result\n",
    "            TJs_vec_proj[TJ_ID][TN_idx, DJ_idx, :] = tangent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualize the projections\n",
    "\n",
    "@interact(TJ_ID=list(TJs_DNs_proj.keys()),\n",
    "          TN_idx=(0,num_ts-1,1))\n",
    "def plot_proj(TJ_ID=list(TJs_DNs_proj.keys())[0],\n",
    "              TN_idx=num_ts//2):\n",
    "    \n",
    "    # Prep plot\n",
    "    plt.figure(figsize=(6,6))\n",
    "    \n",
    "    # For each adjacent DJ...\n",
    "    for DJ_idx, DJ_ID in enumerate(list(itertools.combinations(TJ_ID, 2))):\n",
    "        \n",
    "        # Plot the projected points\n",
    "        plt.scatter(TJs_DNs_proj[TJ_ID][DJ_ID][TN_idx][:,1],\n",
    "                    TJs_DNs_proj[TJ_ID][DJ_ID][TN_idx][:,0],\n",
    "                    c=TJs_DNs_dist[TJ_ID][DJ_ID][TN_idx],\n",
    "                    cmap='viridis', alpha=0.5, lw=0)\n",
    "        \n",
    "        # Plot the fitted vectors\n",
    "        plt.plot([0, TJs_vec_proj[TJ_ID][TN_idx, DJ_idx, 1]*10], \n",
    "                 [0, TJs_vec_proj[TJ_ID][TN_idx, DJ_idx, 0]*10],\n",
    "                 c='k', lw='2', alpha=0.75)\n",
    "    \n",
    "    # Finish\n",
    "    plt.xlabel('u')\n",
    "    plt.ylabel('v')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aligning Incident Vectors Along TJs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Consensus Incident Vector Triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving the Force Balance Equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3base]",
   "language": "python",
   "name": "conda-env-py3base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
