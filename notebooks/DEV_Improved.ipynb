{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pyCFI Development Notebook - Improved Full 3D Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "- Open issues are marked in comments using the word `FLAG`\n",
    "\n",
    "\n",
    "- **Chages/improvements in `VAL_Improved` that have not (yet) been ported here:**\n",
    "    - Add the `min_DNs` constraint\n",
    "    - Wherever DJ_IDs are generated by `itertools.combinations` of TJ_IDs, skip those DJ_IDs that have been removed!\n",
    "    - Correct `<= min_TNs` to `< min_TNs` where necessary\n",
    "    - Add `figsize=(4,4)` and `plt.{x|y}lim([-1.1, 1.1])` to all three vector plotting loops\n",
    "    - Adjust the query range for finding TN-adjacent points during arc fitting to the resolution!\n",
    "    - Convert TJs and DJs defaultdicts to dicts and add appropriate handling\n",
    "    - Projection: Removal of TNs where only very few or only very closely bunched up neighbors are found for projection\n",
    "    - Arc fitting: Removal of TNs where the arc fitting erroneously produces an \"inscribed\" circle\n",
    "    - Arc fitting: Get rid of the overly specific straight line fix\n",
    "    - Improve the TJ spline visualization to keep the same colors as in the previous 3D plot\n",
    "    - More angle wrapping issues:\n",
    "        - Copy over the `wrap_median` function\n",
    "        - Use it instead of `np.median` wherever appropriate (and change name from `median` to `median_ang`)\n",
    "        - Use `wrap_sub` for initial zeroing during triplet alignment\n",
    "    - Add comments to parameters/settings\n",
    "    - Add the new consensus calculation and the max deviation treshold\n",
    "    - Add dropping of columns from `G` (whilst handling relevant 'linked' objects)!\n",
    "    - During CellFIT data loading, add selector of `t4` vs `t9` based on `fpath`\n",
    "    - Add the `nan` fill-in before arc fitting and then add the nan-removal cell afterwards\n",
    "    - Add the use of `proxy_TN` during arc fitting (as well as during centroid fitting and in the subsequent plot)\n",
    "    - Switch the coordinate offset from 0.5 to 1.0\n",
    "    - Switch the centroid fit to skipping as a default and handle `num_ts`/nan-removal appropriately!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports\n",
    "\n",
    "import itertools, collections\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "from skimage import io\n",
    "from scipy import spatial\n",
    "from scipy import interpolate\n",
    "from scipy import optimize\n",
    "import sympy as sym\n",
    "\n",
    "from ipywidgets import interact\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parameters & Settings\n",
    "\n",
    "fpath = r'../Data/Generated/three_intersecting_spheres_aniso.tif'\n",
    "res   = np.array([1.0, 0.5, 0.5])  # Voxel sizes (z,y,x) in microns\n",
    "min_TNs = 3\n",
    "min_z   = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load input segmentation stack\n",
    "\n",
    "im = io.imread(fpath)\n",
    "print(im.dtype, im.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Show input stack\n",
    "\n",
    "@interact(z=(0, im.shape[0]-1, 1))\n",
    "def show_stack(z=im.shape[0]//2):\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.imshow(im[z], cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying Object Outlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Identify outline voxels by comparing shifted images\n",
    "\n",
    "# Pad the image by 1 voxel on all sides\n",
    "im_pad = np.pad(im, 1, mode='reflect')\n",
    "\n",
    "# Get possible shifts in all directions\n",
    "shifts = itertools.product([0,1], repeat=3)\n",
    "\n",
    "# Check and accumulate differences in shifts\n",
    "outlines = np.zeros_like(im, dtype=np.bool)\n",
    "for shift in shifts:\n",
    "    zs0, ys0, xs0 = [slice(1, None) if s else slice(None) for s in shift]\n",
    "    zs1, ys1, xs1 = [slice(None,-1) if s else slice(None) for s in shift]\n",
    "    comparison = im_pad[zs0, ys0, xs0] != im_pad[zs1, ys1, xs1]\n",
    "    outlines  += comparison[:im.shape[0],  :im.shape[1],  :im.shape[2]]\n",
    "    outlines  += comparison[-im.shape[0]:, -im.shape[1]:, -im.shape[2]:]  # Symmetry\n",
    "    \n",
    "# Re-annotate the cell identities\n",
    "outlines_id = outlines * im\n",
    "\n",
    "# Report\n",
    "print(outlines.dtype, outlines.shape)\n",
    "print(outlines_id.dtype, outlines_id.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Show identified outlines\n",
    "\n",
    "@interact(z=(0, im.shape[0]-1, 1))\n",
    "def show_stack(z=im.shape[0]//2):\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.imshow(outlines_id[z], cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying Triple Nodes (TNs) and Triple Junctions (TJs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Find coordinates of all voxels involved in triple junctions\n",
    "\n",
    "# FLAG: PERFORMANCE -- This could potentially be done with image shifting much like `outlines` above!\n",
    "# FLAG: PRECISION -- For the coordinates, would a `+1.0` be more appropriate at interfaces between two cells?\n",
    "\n",
    "# Get Outline Indices (OIs) and Outline Coordinates (OCs)\n",
    "OIs = np.array(np.where(outlines)).T\n",
    "OCs = (OIs + 0.5) * res\n",
    "\n",
    "# Go through OIs and find TN Indices (TNIs)\n",
    "TNIs = []\n",
    "for OI in OIs:\n",
    "    selection = im_pad[OI[0]+1:OI[0]+3, OI[1]+1:OI[1]+3, OI[2]+1:OI[2]+3]\n",
    "    if len(set(selection.flatten())) == 3:\n",
    "        TNIs.append(OI)\n",
    "TNIs = np.array(TNIs)\n",
    "\n",
    "# Convert to TN Coordinates (TNCs)\n",
    "TNCs = (TNIs + 0.5) * res\n",
    "\n",
    "# Report\n",
    "print('OCs: ', OCs.shape)\n",
    "print('TNIs:', TNIs.shape)\n",
    "print('TNCs:', TNCs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Build a dict of TJs structured as: {tuple(cell1_ID, cell2_ID, cell3_ID) : array(INDICES INTO TNIs/TNCs)}\n",
    "\n",
    "# Prepare defaultdict\n",
    "TJs = collections.defaultdict(lambda : [])\n",
    "\n",
    "# Go through TNs, create IDs, assign coordinates to IDs\n",
    "for idx,TNI in enumerate(TNIs):\n",
    "    selection = im_pad[np.int(TNI[0])+1:np.int(TNI[0])+3, \n",
    "                       np.int(TNI[1])+1:np.int(TNI[1])+3, \n",
    "                       np.int(TNI[2])+1:np.int(TNI[2])+3]\n",
    "    TJ_ID = tuple(sorted(set(selection.flatten())))\n",
    "    TJs[TJ_ID].append(idx)\n",
    "\n",
    "# Convert TJ lists to numpy arrays & remove unwanted\n",
    "for TJ_ID in list(TJs.keys()):\n",
    "    \n",
    "    # Remove if too short\n",
    "    if len(TJs[TJ_ID]) <= min_TNs:\n",
    "        del TJs[TJ_ID]\n",
    "        continue\n",
    "        \n",
    "    # Remove if not across >min_Z z-slices\n",
    "    if np.unique(TNIs[TJs[TJ_ID]][:,0]).size < min_z:\n",
    "        del TJs[TJ_ID]\n",
    "        continue\n",
    "    \n",
    "    # Convert to array\n",
    "    TJs[TJ_ID] = np.array(TJs[TJ_ID])\n",
    "    \n",
    "# Report\n",
    "print('TJs:', len(TJs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Show identified TJs on image stack\n",
    "\n",
    "@interact(z=(0, im.shape[0]-1, 1))\n",
    "def show_stack(z=im.shape[0]//2):\n",
    "    \n",
    "    # Prep and plot image\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.imshow(outlines_id[z], cmap='gray')\n",
    "    \n",
    "    # For each TJ...\n",
    "    for TJ_num,TJ_ID in enumerate(TJs.keys()):\n",
    "        \n",
    "        # Get the TJ's TNs in the selected z plane\n",
    "        TNs_in_plane = TNIs[TJs[TJ_ID]][TNIs[TJs[TJ_ID]][:,0]==z]\n",
    "        \n",
    "        # Plot the points\n",
    "        plt.scatter(TNs_in_plane[:, 2], TNs_in_plane[:, 1],\n",
    "                    c=[TJ_num for _ in range(TNs_in_plane.shape[0])], # Coloring trick!\n",
    "                    cmap='hsv', vmin=0, vmax=len(TJs), s=20)\n",
    "        \n",
    "    # Finish\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Show identified TJs as 3D scatter\n",
    "\n",
    "# Prepare the plot\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "ax  = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot each TJ in a different color\n",
    "for TJ_num,TJ_ID in enumerate(TJs.keys()):\n",
    "    ax.scatter(TNCs[TJs[TJ_ID]][:,2], TNCs[TJs[TJ_ID]][:,1], TNCs[TJs[TJ_ID]][:,0],\n",
    "               c=[TJ_num for _ in range(TJs[TJ_ID].shape[0])], \n",
    "               cmap='hsv', vmin=0, vmax=len(TJs), s=10)\n",
    "\n",
    "## Also show cell outlines [may take several seconds to render!]\n",
    "#ax.scatter([c[2] for c in OCs],\n",
    "#           [c[1] for c in OCs],\n",
    "#           [c[0] for c in OCs],\n",
    "#           c='gray', alpha=0.01, linewidth=0, s=5)\n",
    "\n",
    "## Axis limits\n",
    "#ax.set_xlim([0,200])\n",
    "#ax.set_ylim([0,200])\n",
    "#ax.set_zlim([0,200])\n",
    "\n",
    "# Finish\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying Double Nodes (DNs) and Double Junctions (DJs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Find coordinates of all voxels involved in DOUBLE junctions\n",
    "\n",
    "# FLAG: PERFORMANCE -- Same as for TNI/TNC extraction above!\n",
    "# FLAG: PRECISION -- Same as for TNI/TNC extraction above!\n",
    "\n",
    "# Go through OIs and find DN Indices (DNIs)\n",
    "DNIs = []\n",
    "for OI in OIs:\n",
    "    selection = im_pad[OI[0]+1:OI[0]+3, \n",
    "                       OI[1]+1:OI[1]+3, \n",
    "                       OI[2]+1:OI[2]+3]\n",
    "    if len(set(selection.flatten())) == 2:\n",
    "        DNIs.append(OI)\n",
    "DNIs = np.array(DNIs)\n",
    "\n",
    "# Convert to DN Coordinates (DNCs)\n",
    "DNCs = (DNIs + 0.5) * res\n",
    "\n",
    "# Report\n",
    "print('OCs: ', OCs.shape)\n",
    "print('DNIs:', DNIs.shape)\n",
    "print('DNCs:', DNCs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Build a dict of Double Junctions (DJs) structured as: {tuple(cell1_ID, cell2_ID) : array(INDICES INTO DNIs/DNCs)}\n",
    "\n",
    "# Prepare defaultdict\n",
    "DJs = collections.defaultdict(lambda : [])\n",
    "\n",
    "# Go through DNs, create IDs, assign coordinates to IDs\n",
    "for idx,DNI in enumerate(DNIs):\n",
    "    selection = im_pad[np.int(DNI[0])+1:np.int(DNI[0])+3, \n",
    "                       np.int(DNI[1])+1:np.int(DNI[1])+3, \n",
    "                       np.int(DNI[2])+1:np.int(DNI[2])+3]\n",
    "    DJ_ID = tuple(sorted(set(selection.flatten())))\n",
    "    DJs[DJ_ID].append(idx)\n",
    "\n",
    "# Convert DJ lists to numpy arrays & remove unwanted\n",
    "for DJ_ID in list(DJs.keys()):\n",
    "    \n",
    "    # Remove if not across >min_z z-slices\n",
    "    if np.unique(DNIs[DJs[DJ_ID]][:,0]).size < min_z:\n",
    "        del DJs[DJ_ID]\n",
    "        continue\n",
    "    \n",
    "    # Convert to array\n",
    "    DJs[DJ_ID] = np.array(DJs[DJ_ID])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Show identified DJs on image stack\n",
    "\n",
    "@interact(z=(0, im.shape[0]-1, 1))\n",
    "def show_stack(z=im.shape[0]//2):\n",
    "    \n",
    "    # Prep and plot image\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.imshow(outlines_id[z], cmap='gray')\n",
    "    \n",
    "    # For each DJ...\n",
    "    for DJ_num,DJ_ID in enumerate(DJs.keys()):\n",
    "        \n",
    "        # Get the DJ's DNs in the selected z plane\n",
    "        DNs_in_plane = DNIs[DJs[DJ_ID]][DNIs[DJs[DJ_ID]][:,0]==z]\n",
    "        \n",
    "        # Plot the points\n",
    "        plt.scatter(DNs_in_plane[:, 2], DNs_in_plane[:, 1],\n",
    "                    c=[DJ_num for _ in range(DNs_in_plane.shape[0])], # Coloring trick!\n",
    "                    cmap='hsv', vmin=0, vmax=len(DJs), s=5, lw=0, alpha=0.5)\n",
    "        \n",
    "    # Finish\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting Splines to TJs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns out spline fitting requires the input points to be roughly in order along the spline, which isn't guaranteed in our case. Ordering the points happens to be far harder problem than one might imagine (it's a variation of traveling salesman) but luckily it can be solved quite well with a Breadth-First Search (BFS). This solution is partially inspired by Imanol Luengo's answer to [this SO question](https://stackoverflow.com/questions/37742358/sorting-points-to-form-a-continuous-line).\n",
    "\n",
    "<font color=orange>**Warning 1:**</font> This will fail for geometries that exhibit \"crossings\" or \"forks\" of any kind. Although that should be very rare/non-existent in the data, a special form of \"fork\" is the circle. In case of a fully circular TJ, which occurs when two cells neatly touch each other, this will fail (unless some points are removed from the TJ). I couldn't come up with a way of fixing this but divised the `InvalidPathError` to at least pick up on such cases. However, **it may be too stringent** as it is currently implemented!\n",
    "\n",
    "<font color=orange>**Warning 2:**</font> Simply rescaling the z axis a little bit already led to renewed problems with this approach, so I'm starting to seriously doubt its robustness. We'll have to keep a close eye on this and possibly somehow develop a better solution if problems keep cropping up. Maybe some sort of modified graph search (rather than straight up BFS) would be a possibility..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function to reorder TEs along the progression of the TJ\n",
    "\n",
    "# FLAG: ROBUSTNESS -- I still have my doubts as to the robustness of this approach (see warnings above)!\n",
    "#                     I keep wondering if there isn't a better way!\n",
    "\n",
    "# Define helpful custom exceptions\n",
    "class InvalidGraphError(Exception): pass\n",
    "class InvalidPathError(Exception): pass\n",
    "\n",
    "# Define function\n",
    "def sort_line_coords(coords, N_neighbors=10, source=None, \n",
    "                     return_argsort=False, ignore_path_check=False):\n",
    "    \"\"\"Given a set of coordinates that roughly lie on a 1D curve in mD space\n",
    "    (but may be in random order), sort the points such that they roughly follow \n",
    "    the curve's progression.\n",
    "    \n",
    "    Uses a breadth-first search tree on a nearest-neighbor graph of the coords,\n",
    "    which happens to result in the best possible sort. Does not work as intended\n",
    "    for closed curves and curves that form any kind of fork or crossing; an \n",
    "    Error is raised in such cases.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    coords : array of shape (N_points, M_dimensions)\n",
    "        Coordinates of points roughly lying on a point in M-dimensional space.\n",
    "    N_neighbors : int, optional, default 10\n",
    "        Number of nearest neighbors to include for each graph. If this is set\n",
    "        too low, connected components may form and no complete solution is\n",
    "        possible (raises an Exception). If this is set too high, the resulting\n",
    "        sort is very imprecises. The ideal value must be determined empirically.\n",
    "        When used to prepare TJs for spline fitting in the context of pyCFI, the\n",
    "        default (10) is a reasonably choice and the outcome is largely robust\n",
    "        to changes between values of 5 and 20.\n",
    "    source : None or int, optional, default None\n",
    "        The source is a point at one of the two ends of the line. If None, the\n",
    "        point is automatically determined by testing all different points and \n",
    "        selecting the one that yields the best sort (by minimizing the resulting\n",
    "        path distance). If source is an int, it indexes into coords to specify\n",
    "        the end point from which the sort is constructed. This saves a lot of\n",
    "        time compared to the automated search, especially if there are many\n",
    "        points, however it requires prior knowledge of the end point.\n",
    "    return_argsort : bool, optional, default False\n",
    "        If True, the index array that sorts the points into the best order is \n",
    "        returned as a second result. Otherwise, only a sorted version of coords \n",
    "        is returned.\n",
    "    ignore_path_check : bool, optional, default False\n",
    "        If True, the final path is not cross-checked and no InvalidPathErrors\n",
    "        can be raised (see Exceptions below).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    sorted_coords : array of shape (N_points, M_dimensions)\n",
    "        The same set of points as in the input coords but sorted along the\n",
    "        curve's progression in space.\n",
    "    best_path : array of shape (N_points,)\n",
    "        Index array that sorts points along the curve's progression in space. \n",
    "        Only returned if return_argsort is set to True.\n",
    "        \n",
    "    Exceptions\n",
    "    ----------\n",
    "    InvalidGraphError : If the adjacency graph created based on the kdTree is\n",
    "        not fully connected, InvalidGraphError is raised. This may imply that\n",
    "        N_neighbors is too low or that the points in coords do not belong to\n",
    "        a single continuous line.\n",
    "    InvalidPathError : If the curve is closed or contains forks/crossings, the\n",
    "        sort fails, which is reflected in the fact that the final path will\n",
    "        contain steps that do not have corresponding edges on the graph. In\n",
    "        this case, InvalidPathError is raised. This may also occur under other\n",
    "        dubious circumstances, e.g. if the input data is not a curve at all \n",
    "        or if it is a very broad curve or if N_neighbors is too low.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get pairwise distances (if needed)\n",
    "    if source is None:\n",
    "        dists = spatial.distance.squareform(spatial.distance.pdist(coords))\n",
    "    \n",
    "    # Get nearest neighbors\n",
    "    kdtree  = spatial.cKDTree(coords)\n",
    "    _, KNNs = kdtree.query(coords, k=N_neighbors if N_neighbors<coords.shape[0] else coords.shape[0])\n",
    "    \n",
    "    # Build adjacency matrix\n",
    "    adj_M = np.zeros((coords.shape[0], coords.shape[0]), dtype=np.bool)\n",
    "    for i,N in enumerate(KNNs):\n",
    "        adj_M[i,N] = True\n",
    "    \n",
    "    # Construct networkx graph\n",
    "    G = nx.from_numpy_array(adj_M)\n",
    "    if not nx.is_connected(G):\n",
    "        #class InvalidGraphError(Exception): pass\n",
    "        raise InvalidGraphError('Adjacency graph is not fully connected!')\n",
    "     \n",
    "    # If a source node is given, just get its BFS tree\n",
    "    if source is not None:\n",
    "        best_path = list(nx.bfs_tree(G, source))\n",
    "        \n",
    "    # Otherwise, find the best BFS tree from all sources\n",
    "    if source is None:\n",
    "        paths = []\n",
    "        costs = []\n",
    "        for n in G.nodes():\n",
    "\n",
    "            # Get BFS tree\n",
    "            path = list(nx.bfs_tree(G, n))\n",
    "\n",
    "            # Get sum of all distances within tree\n",
    "            cost = 0.0\n",
    "            for n0,n1 in zip(path, path[1:]):\n",
    "                cost += dists[n0, n1]\n",
    "\n",
    "            # Keep results\n",
    "            paths.append(path)\n",
    "            costs.append(cost)\n",
    "\n",
    "        # Select the best solution\n",
    "        best_path = paths[np.argmin(costs)]\n",
    "    \n",
    "    # Test for cases that probably failed\n",
    "    if not ignore_path_check:\n",
    "        for p1,p2 in zip(best_path, best_path[1:]):\n",
    "            if not G.has_edge(p1,p2):\n",
    "                raise InvalidPathError(\"The sort path uses an edge that is not on the graph. \"+\n",
    "                                       \"This should not happen and probably implies that the \"+\n",
    "                                       \"curve is cyclical or has a fork/crossing.\")\n",
    "    \n",
    "    # Sort coords and return\n",
    "    if return_argsort:\n",
    "        return coords[best_path], best_path\n",
    "    else:\n",
    "        return coords[best_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### A quick test of the TJ sorting\n",
    "\n",
    "# FLAG: ROBUSTNESS -- Currently, ingore_path_check has to be set to True for this to work\n",
    "#                     when z is properly rescaled (although the sort overall actually\n",
    "#                     doesn't look too bad).\n",
    "\n",
    "# Grab a TJ and compute the sort\n",
    "TJCs = TNCs[TJs[list(TJs.keys())[0]]]\n",
    "sorted_TJCs = sort_line_coords(TJCs, ignore_path_check=True)\n",
    "\n",
    "# Get sorted pairwise distances\n",
    "sorted_dists = spatial.distance.squareform(spatial.distance.pdist(sorted_TJCs))\n",
    "\n",
    "# Plot pairwise distances\n",
    "fig, ax = plt.subplots(1, 2, figsize=(13,6.5))\n",
    "ax[0].imshow(sorted_dists)\n",
    "ax[0].set_title(\"sorted distances\")\n",
    "ax[0].set_xlabel('TNs'); ax[0].set_ylabel('TNs')\n",
    "\n",
    "# Plot path/tree on scatter\n",
    "ax[1].scatter(TJCs[:,1], TJCs[:,0], s=20, alpha=0.5)\n",
    "ax[1].plot(sorted_TJCs[:,1], sorted_TJCs[:,0], c='r', alpha=0.75)\n",
    "ax[1].set_title(\"path scatter\")\n",
    "ax[1].set_xlabel('y'); ax[1].set_ylabel('z')\n",
    "\n",
    "# Done\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Wrapper for spline fitting\n",
    "\n",
    "def wrap_splprep(coords, k=3, verbose=False):\n",
    "    \"\"\"Fit an nD spline with scipy.interpolate.splprep.\n",
    "    \n",
    "    coords : array (points, dimensions) : input data\n",
    "    k=3 : integer : degrees of freedom\n",
    "    verbose=False : bool : wether to print all outputs\n",
    "    \n",
    "    returns -> tck : tuple (knots, coefficients, k) : \n",
    "               fit parameters as used by splev\n",
    "    \"\"\"\n",
    "    \n",
    "    # Fit the spline and unpack the (weirdly packaged) results\n",
    "    tcku, fp, ier, msg = interpolate.splprep(coords.T, k=k, full_output=True)\n",
    "    tck, u = tcku\n",
    "\n",
    "    # Report the results\n",
    "    if verbose:\n",
    "        print ('\\nt (knots, tck[0]):\\n' , tck[0])\n",
    "        print ('\\nc (coefficients, tck[1]):\\n' , tck[1])\n",
    "        print ('\\nk (degree, tck[2]):' , tck[2])\n",
    "        print ('\\nu (evaluation points):\\n', u)\n",
    "        print ('\\nfp (residual error):', fp)\n",
    "        print ('\\nier (error code; success is ier<=0):', ier)\n",
    "        print ('\\nmsg (message from FITPACK):\\n', msg)\n",
    "        \n",
    "    # Raise an error if FITPACK indicates failure\n",
    "    if ier > 0:\n",
    "        raise Exception('ier is >0, indicating that FITPACK failed somehow. '+\n",
    "                        'The message from FITPACK was:\\n'+msg)\n",
    "        \n",
    "    # Return the only result relevant to spline evaluation\n",
    "    return tck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Perform sorting and spline fitting on all TJs\n",
    "\n",
    "# FLAG -- PRECISION: Currently, cases where the TNs of a single TJ_ID do not form a single\n",
    "#                    continuous line are caught and those TJs are removed entirely (see\n",
    "#                    InvalidGraphError handling). However, such cases can naturally occur\n",
    "#                    in some (rare-ish) geometries involving 4+ cells and the background.\n",
    "#                    Would be nice to somehow recognize these cases and handle them better,\n",
    "#                    though the way the TJ_IDs are currently done wouldn't readily allow\n",
    "#                    such a solution...\n",
    "\n",
    "# Parameters\n",
    "num_ts = 20     # Determines the number of TNs that will be analyzed throughout the rest of the pipeline\n",
    "tng_dv = 10e-2  # FLAG -- PRECISION: Should this be smaller? FLAG -- ROBUSTNESS: Should this scale with res?\n",
    "\n",
    "# Output dicts\n",
    "TJs_spline_tck     = {}  # Fitted splines for each TJ\n",
    "TJs_spline_t       = {}  # Parameter (t) values for evaluation\n",
    "TJs_spline_ev      = {}  # Evaluated splines (at each t) for each TJ\n",
    "TJs_spline_tangent = {}  # Tangents to splines for each TJ\n",
    "\n",
    "# For each TJ...\n",
    "for TJ_ID in TJs.keys():\n",
    "    \n",
    "    # Sort coordinates along the line\n",
    "    try:\n",
    "        sorted_TJCs, TJ_argsort = sort_line_coords(TNCs[TJs[TJ_ID]],\n",
    "                                                   return_argsort=True,\n",
    "                                                   ignore_path_check=True)\n",
    "    except InvalidGraphError:  # Remove cases where points with the same TJ...\n",
    "        del TJs[TJ_ID]         # ...identifier don't form a continuous line.\n",
    "        continue\n",
    "    TJs[TJ_ID] = TJs[TJ_ID][TJ_argsort]\n",
    "    \n",
    "    # Perform spline fitting\n",
    "    tck = wrap_splprep(sorted_TJCs)\n",
    "    TJs_spline_tck[TJ_ID] = tck\n",
    "    \n",
    "    # Evaluate the spline in 1000 regular intervals\n",
    "    TJs_spline_t[TJ_ID] = np.linspace(0.0, 1.0, num_ts)\n",
    "    ev = interpolate.splev(TJs_spline_t[TJ_ID], tck)\n",
    "    ev = np.array(ev).T\n",
    "    TJs_spline_ev[TJ_ID] = ev\n",
    "    \n",
    "    # Also evaluate with slight deviation forward and backward\n",
    "    evD1 = np.array(interpolate.splev(TJs_spline_t[TJ_ID]+tng_dv, tck)).T\n",
    "    evD2 = np.array(interpolate.splev(TJs_spline_t[TJ_ID]-tng_dv, tck)).T\n",
    "    \n",
    "    # Approximate the tangent vector as the sum of the deviatory vectors\n",
    "    tangent_vec = ((evD1 - ev) + (ev - evD2)) / 2.0\n",
    "    TJs_spline_tangent[TJ_ID] = tangent_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualize the fitted splines and the tangent vectors as 3D scatter\n",
    "\n",
    "# Prepare the plot\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "ax  = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot each TJ spline\n",
    "for TJ_num,TJ_ID in enumerate(TJs.keys()):\n",
    "    ax.plot(TJs_spline_ev[TJ_ID][:,2], \n",
    "            TJs_spline_ev[TJ_ID][:,1], \n",
    "            TJs_spline_ev[TJ_ID][:,0],\n",
    "            lw=3)\n",
    "\n",
    "# Add the tangent vectors\n",
    "for TJ_num, TJ_ID in enumerate(TJs.keys()):\n",
    "    for splpt, tngvec in zip(TJs_spline_ev[TJ_ID][::5], TJs_spline_tangent[TJ_ID][::5]):\n",
    "        plt.plot([splpt[2], splpt[2]+tngvec[2]],\n",
    "                 [splpt[1], splpt[1]+tngvec[1]],\n",
    "                 [splpt[0], splpt[0]+tngvec[0]],\n",
    "                 'r-', alpha=0.5)\n",
    "            \n",
    "## Axis limits\n",
    "#ax.set_xlim([0,200])\n",
    "#ax.set_ylim([0,200])\n",
    "#ax.set_zlim([0,200])\n",
    "\n",
    "# Finish\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projecting DNs onto TJ-Orthogonal Dihedral Planes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sympy function to project close-by outline points onto a TN's TJ-orthogonal plane\n",
    "\n",
    "# FLAG: PERFORMANCE -- Save the resulting numpy func so that the symbolic solving doesn't need \n",
    "#                      to be rerun each time the code is executed! This is probably best done\n",
    "#                      by copying the function out into a .py file and importing it from there.\n",
    "#                      In the process, axis keywords could perhaps be added to handle vectorized\n",
    "#                      execution across many points/planes (see issue flag below).\n",
    "# FLAG: ROBUSTNESS -- Simply doing Gram-Schmidt as we currently do does not preserve the\n",
    "#                     uv-coordinate system within the plane across multiple TNs of a TJ. \n",
    "#                     Under certain circumstances (when values of the normal vector cross\n",
    "#                     zero), this can even lead to sudden 'flipping' of the orientation of\n",
    "#                     the plane. Currently, this is implicitly being \"fixed\" downstream \n",
    "#                     since the vector triplets are being aligned by rotation and flipping\n",
    "#                     prior to their reduction to a consensus triplet. However, it might be\n",
    "#                     more clean and robust to do something slightly more sophisticated than\n",
    "#                     classical Gram-Schmidt in order to enforce consistency.\n",
    "\n",
    "# Import sympy symbols\n",
    "from sympy.abc import q,r,s,  x,y,z  # (normal vector), (point to be projected)\n",
    "\n",
    "# Use Gram-Schmidt orthogonalization to create orthonormal vectors defining the in-plane\n",
    "# coordinate system given three arbitrary vectors, the first of which is the normal vector\n",
    "# of the plane. The other two (defining the in-plane directionalities) are arbitrarily \n",
    "# chosen such that they will never fall onto the normal vector or onto each other.\n",
    "orthonormals = sym.GramSchmidt([sym.Matrix([q,         r,         s]),  # Normal vec to plane -> first coordinate vec\n",
    "                                sym.Matrix([q, 2*(r+0.1), 3*(s+0.1)]),  # Arbitrary vec not on the normal vec\n",
    "                                sym.Matrix([2*(q+0.1), 3*(r+0.1), s])], # Arbitrary vec not on either other vec\n",
    "                                orthonormal=True)           # Normalize resulting orthogonal vectors\n",
    "\n",
    "# With the resulting orthonormals defining the new coordinate system, the projection\n",
    "# of points into it is just a straightforward dot product.\n",
    "projection_pt = sym.Matrix([x, y, z])\n",
    "proj_d = orthonormals[0].dot(projection_pt)  # Distance from plane\n",
    "proj_u = orthonormals[1].dot(projection_pt)  # Coordinate along first axis in plane\n",
    "proj_v = orthonormals[2].dot(projection_pt)  # Coordinate along second axis in plane\n",
    "\n",
    "# Lambdify\n",
    "lambda_dist = sym.utilities.lambdify((q,r,s,x,y,z), proj_d, modules='numpy')\n",
    "lambda_u    = sym.utilities.lambdify((q,r,s,x,y,z), proj_u, modules='numpy')\n",
    "lambda_v    = sym.utilities.lambdify((q,r,s,x,y,z), proj_v, modules='numpy')\n",
    "\n",
    "# Wrap into a function (sequential)\n",
    "def p2p_projection(normal_vec, pt_coords):\n",
    "        \n",
    "    # Unpack inputs\n",
    "    q,r,s = normal_vec[2], normal_vec[1], normal_vec[0]\n",
    "    x,y,z = pt_coords[:,2], pt_coords[:,1], pt_coords[:,0]\n",
    "    \n",
    "    # Run projection\n",
    "    dists = np.abs(lambda_dist(q,r,s,x,y,z))\n",
    "    p_u   = lambda_u(q,r,s,x,y,z)\n",
    "    p_v   = lambda_v(q,r,s,x,y,z)\n",
    "    \n",
    "    # Pack and return outputs\n",
    "    projected = np.array([p_u, p_v]).T\n",
    "    return projected, dists\n",
    "\n",
    "## Wrap into a function (vectorized)\n",
    "## FLAG -- ISSUE: This does not work as intended! It runs but does not yield the same results\n",
    "##                as the sequential version. There is likely an missing `axis=` kwarg in one \n",
    "##                of the numpy functions substituted by lambdify. This could perhaps be fixed\n",
    "##                by manual inspection of the projection function.\n",
    "#def p2p_projection_vectorized(normal_vec, pt_coords):\n",
    "#    \n",
    "#    # Unpack inputs\n",
    "#    q,r,s = normal_vec[..., 2, np.newaxis], normal_vec[..., 1, np.newaxis], normal_vec[..., 0, np.newaxis]\n",
    "#    x,y,z = pt_coords[..., 2], pt_coords[..., 1], pt_coords[..., 0]\n",
    "#    \n",
    "#    # Run projection\n",
    "#    dists = np.abs(lambda_dist(q,r,s,x,y,z))\n",
    "#    p_u   = lambda_u(q,r,s,x,y,z)\n",
    "#    p_v   = lambda_v(q,r,s,x,y,z)\n",
    "#    \n",
    "#    # Pack and return outputs\n",
    "#    projected = np.rollaxis(np.array([p_u, p_v]), 2)\n",
    "#    projected = np.rollaxis(projected, 2)\n",
    "#    return projected, dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Project relevant DNs onto the TJ-orthogonal plane\n",
    "\n",
    "# FLAG: PERFORMANCE -- This seems to scale very poorly! It takes a long time to run for\n",
    "#                      a dataset that is just slightly bigger than the test data. Find\n",
    "#                      ways of mitigating this, in particular by getting the vectorized\n",
    "#                      version of p2p to work and maybe also by parallelization.\n",
    "\n",
    "# Params\n",
    "close_points_radius =  25.0\n",
    "dist_points_keep    = 100\n",
    "\n",
    "# Prep output dicts\n",
    "TJs_DNs_proj = {}\n",
    "TJs_DNs_dist = {}\n",
    "\n",
    "# For each TJ...\n",
    "for TJ_ID in TJs_spline_ev.keys():\n",
    "    \n",
    "    # Find the IDs of the three connected interfaces\n",
    "    DJ_IDs = list(itertools.combinations(TJ_ID, 2))\n",
    "    \n",
    "    # Skip edge cases with more than 3\n",
    "    if len(DJ_IDs) > 3:\n",
    "        continue\n",
    "        \n",
    "    # Get corresponding TJ-normal vectors\n",
    "    proj_tangents = TJs_spline_tangent[TJ_ID]\n",
    "    \n",
    "    # Prep output lists\n",
    "    TJs_DNs_proj[TJ_ID] = {DJ_ID:[] for DJ_ID in DJ_IDs}\n",
    "    TJs_DNs_dist[TJ_ID] = {DJ_ID:[] for DJ_ID in DJ_IDs}\n",
    "    \n",
    "    # For each TN of the current TJ...\n",
    "    for TN_idx, TN in enumerate(TJs_spline_ev[TJ_ID]): \n",
    "        \n",
    "        # For each connected interface...\n",
    "        for DJ_idx, DJ_ID in enumerate(DJ_IDs):\n",
    "            \n",
    "            # Get all the DJ points of that interface\n",
    "            current_DNCs = DNCs[DJs[DJ_ID]]\n",
    "            \n",
    "            # If there are none, skip this TN\n",
    "            if current_DNCs.size == 0:\n",
    "                print(\"Skipped case at TJ_ID=\"+str(TJ_ID) + \", TN_idx=\" +str(TN_idx) + \n",
    "                      \", DJ_ID=\"+str(DJ_ID)+\" ->> lacks interface points!\")\n",
    "                TJs_DNs_proj[TJ_ID][DJ_ID].append(np.empty(0))\n",
    "                continue\n",
    "                \n",
    "            # Get the DN points close to the TN\n",
    "            kdtree = spatial.cKDTree(current_DNCs)\n",
    "            KNNs   = kdtree.query_ball_point(TN, close_points_radius)\n",
    "            \n",
    "            # If there are none, skip this TN\n",
    "            if not KNNs:\n",
    "                print(\"Skipped case at TJ_ID=\"+str(TJ_ID) + \", TN_idx=\" +str(TN_idx) + \n",
    "                      \", DJ_ID=\"+str(DJ_ID)+\" ->> no close-by neighbors!\")\n",
    "                TJs_DNs_proj[TJ_ID][DJ_ID].append(np.empty(0))\n",
    "                continue\n",
    "            \n",
    "            # Move the points onto the origin\n",
    "            current_DNCs = current_DNCs[KNNs] - TJs_spline_ev[TJ_ID][TN_idx]\n",
    "            \n",
    "            # Nothing *should* go wrong here - but if it does, first look into\n",
    "            # the way the arbitrary vectors for Gram-Schmidt are generated!\n",
    "            with np.errstate(divide='raise', invalid='raise'):\n",
    "                    \n",
    "                # Project the TN points onto the dihedral plane\n",
    "                projs, dists = p2p_projection(proj_tangents[TN_idx], current_DNCs)\n",
    "            \n",
    "            # Threshold on the distances; keep at most the closest n points\n",
    "            psort = np.argsort(dists)[:dist_points_keep]\n",
    "            projs = projs[psort]\n",
    "            dists = dists[psort]\n",
    "            \n",
    "            # Keep the results\n",
    "            TJs_DNs_proj[TJ_ID][DJ_ID].append(projs)\n",
    "            TJs_DNs_dist[TJ_ID][DJ_ID].append(dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualize the projections\n",
    "\n",
    "@interact(TJ_ID=list(TJs_DNs_proj.keys()),\n",
    "          TN_idx=(0,num_ts-1,1))\n",
    "def plot_proj(TJ_ID=list(TJs_DNs_proj.keys())[0],\n",
    "              TN_idx=num_ts//2):\n",
    "    \n",
    "    # Prep plot\n",
    "    plt.figure(figsize=(6,6))\n",
    "    \n",
    "    # For each adjacent DJ...\n",
    "    for DJ_ID in itertools.combinations(TJ_ID, 2):\n",
    "        \n",
    "        # Plot the projected points\n",
    "        plt.scatter(TJs_DNs_proj[TJ_ID][DJ_ID][TN_idx][:,1],\n",
    "                    TJs_DNs_proj[TJ_ID][DJ_ID][TN_idx][:,0],\n",
    "                    c=TJs_DNs_dist[TJ_ID][DJ_ID][TN_idx],\n",
    "                    cmap='viridis', alpha=0.5, lw=0)\n",
    "    \n",
    "    # Finish\n",
    "    plt.xlabel('u')\n",
    "    plt.ylabel('v')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving Incident Vectors in the Dihedral Plane\n",
    "\n",
    "**Note:** The arc fitting approach taken here is based on the second approach described in [this scipy cookbook entry](https://scipy-cookbook.readthedocs.io/items/Least_Squares_Circle.html). It could probably be further improved by using the third approach, i.e. by explicitly specifying the Jacobian function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Functions for circular arc fitting\n",
    "\n",
    "# FLAG -- PERFORMANCE: The arc fitting approach used here could be sped up by explicitly\n",
    "#                      specifying a Jacobian function, see the markdown note above.\n",
    "\n",
    "# Compute coordinates from angle\n",
    "def circle(r, cx, cy, alpha):\n",
    "    x = r*np.cos(alpha) + cx\n",
    "    y = r*np.sin(alpha) + cy\n",
    "    return np.array([y,x])\n",
    "\n",
    "# Compute radius/radii given a center and a point/multiple points\n",
    "def radius(xc, yc, x, y):\n",
    "    return np.sqrt((x-xc)**2 + (y-yc)**2)\n",
    "\n",
    "# Loss: distance of data points from mean circle\n",
    "def circle_loss(c, x, y):\n",
    "    radii = radius(c[0], c[1], x, y)\n",
    "    return radii - radii.mean()\n",
    "\n",
    "# Subtraction of n1 and n2, wrapping around at minimum and maximum\n",
    "def wrap_sub(n1, n2, minimum=-np.pi, maximum=np.pi):\n",
    "    s = n1 - n2 \n",
    "    try:\n",
    "        s[s<=minimum] = maximum + (s[s<=minimum] - minimum)\n",
    "        s[s>=maximum] = minimum + (s[s>=maximum] - maximum)\n",
    "    except TypeError:\n",
    "        if s <= minimum: s = maximum + (s - minimum)\n",
    "        if s >= maximum: s = minimum + (s - maximum)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Find incident vectors for each TN based on circular arc fitting\n",
    "\n",
    "# FLAG -- ROBUSTNESS: There is still an edge case in this where perfectly straight\n",
    "#                     lines are fit with a completely wrong (very small) circle.\n",
    "#                     Right now, this is handled as a \"silly exception\" for the\n",
    "#                     synthetic test sample, where the middle line between cells is\n",
    "#                     perfectly straight. The hope is that this will never occur\n",
    "#                     in real data - but if it does, the curent handling will\n",
    "#                     almost certainly fail, as it presupposes that the line is\n",
    "#                     not only perfectly straight but also perfectly aligned with\n",
    "#                     one of the image axes.\n",
    "\n",
    "# Prep output dict\n",
    "TJs_vec_proj = {}\n",
    "\n",
    "# For each TJ...\n",
    "for TJ_ID in TJs_DNs_proj.keys():\n",
    "    \n",
    "    # Prepare an appropriate result array\n",
    "    TJs_vec_proj[TJ_ID] = np.empty((TJs_spline_ev[TJ_ID].shape[0], 3, 2))  # Num. of TNs, 3 vectors, 2 dimensions\n",
    "    \n",
    "    # For each adjacent DJ...\n",
    "    for DJ_idx, DJ_ID in enumerate(list(itertools.combinations(TJ_ID, 2))):\n",
    "        \n",
    "        # For each TN along the TJ...\n",
    "        for TN_idx in range(len(TJs_DNs_proj[TJ_ID][DJ_ID])):\n",
    "\n",
    "            # Prep data for fitting\n",
    "            x = TJs_DNs_proj[TJ_ID][DJ_ID][TN_idx][:,1]\n",
    "            y = TJs_DNs_proj[TJ_ID][DJ_ID][TN_idx][:,0]\n",
    "            \n",
    "            # Catch silly exception where all the data is in a line (may cause artifacts)\n",
    "            is_silly_exception = False\n",
    "            if np.allclose(x, x[0]) or np.allclose(y, y[0]):\n",
    "                is_silly_exception = True\n",
    "            \n",
    "            # Fit a circle to the data\n",
    "            center, ier = optimize.leastsq(circle_loss, [np.mean(x), np.mean(y)], args=(x, y))\n",
    "            cx, cy = center\n",
    "            r      = radius(cx, cy, x, y).mean()\n",
    "            \n",
    "            # Get angular position of the TN point (which is the origin in the projection)\n",
    "            TN_alpha = np.arctan2(0.0-cy, 0.0-cx)\n",
    "\n",
    "            # Get correct sign for tangent vector direction\n",
    "            DNs_alpha = wrap_sub(np.arctan2(y-cy, x-cx), TN_alpha)\n",
    "            sign = np.sign(np.mean(DNs_alpha))\n",
    "\n",
    "            # Get tangent vector based on TN angle and small shift\n",
    "            TN_proj = circle(r, cx, cy, TN_alpha)\n",
    "            shifted = circle(r, cx, cy, TN_alpha+10e-5)\n",
    "            tangent = shifted - TN_proj\n",
    "            tangent = tangent * sign\n",
    "\n",
    "            # Handle the silly exception where all the data is in a line\n",
    "            if is_silly_exception:\n",
    "                tangent = np.array([np.mean(y), np.mean(x)])\n",
    "            \n",
    "            # Normalize to magnitude 1\n",
    "            tangent = tangent / np.sqrt(np.sum(tangent**2.0))\n",
    "\n",
    "            # Save the result\n",
    "            TJs_vec_proj[TJ_ID][TN_idx, DJ_idx, :] = tangent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualize the projections\n",
    "\n",
    "@interact(TJ_ID=list(TJs_DNs_proj.keys()),\n",
    "          TN_idx=(0,num_ts-1,1))\n",
    "def plot_proj(TJ_ID=list(TJs_DNs_proj.keys())[0],\n",
    "              TN_idx=num_ts//2):\n",
    "    \n",
    "    # Prep plot\n",
    "    plt.figure(figsize=(6,6))\n",
    "    \n",
    "    # For each adjacent DJ...\n",
    "    for DJ_idx, DJ_ID in enumerate(list(itertools.combinations(TJ_ID, 2))):\n",
    "        \n",
    "        # Plot the projected points\n",
    "        plt.scatter(TJs_DNs_proj[TJ_ID][DJ_ID][TN_idx][:,1],\n",
    "                    TJs_DNs_proj[TJ_ID][DJ_ID][TN_idx][:,0],\n",
    "                    c=TJs_DNs_dist[TJ_ID][DJ_ID][TN_idx],\n",
    "                    cmap='viridis', alpha=0.5, lw=0)\n",
    "        \n",
    "        # Plot the fitted vectors\n",
    "        plt.plot([0, TJs_vec_proj[TJ_ID][TN_idx, DJ_idx, 1]*10], \n",
    "                 [0, TJs_vec_proj[TJ_ID][TN_idx, DJ_idx, 0]*10],\n",
    "                 c='k', lw='2', alpha=0.75)\n",
    "    \n",
    "    # Finish\n",
    "    plt.xlabel('u')\n",
    "    plt.ylabel('v')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Show the resulting vector triplets \n",
    "\n",
    "# FLAT: NOTE -- These are not aligned, which is okay but not ideal;\n",
    "#               see ROBUSTNESS flag in sympy projection code.\n",
    "\n",
    "# For each TJ...\n",
    "cols = ['r','g','b']\n",
    "for TJ_ID in TJs_vec_proj.keys():\n",
    "    \n",
    "    # Prep\n",
    "    plt.figure()\n",
    "    \n",
    "    # Plot each vec...\n",
    "    for vec in TJs_vec_proj[TJ_ID]:\n",
    "        for i,v in enumerate(vec):\n",
    "            plt.plot([0,v[1]], [0,v[0]], c=cols[i])\n",
    "            \n",
    "    # Finalize\n",
    "    plt.title(str(TJ_ID))\n",
    "    plt.xlabel('x'); plt.ylabel('y')\n",
    "    plt.axis('equal')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aligning Incident Vectors Along TJs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Align triplets based on first vector & flip those that are the wrong way around\n",
    "\n",
    "# For each TJ...\n",
    "TJs_vec_aligned = {}\n",
    "for TJ_ID in TJs_vec_proj.keys():\n",
    "    \n",
    "    # Prep output container\n",
    "    triplets_aligned = np.empty_like(TJs_vec_proj[TJ_ID])\n",
    "    \n",
    "    ## Rotate each triplet to lay the first vector onto zero angle [sequential]\n",
    "    #angles_zeroed = np.empty((TJs_vec_proj[TJ_ID].shape[0], 3))\n",
    "    #for t,triplet in enumerate(TJs_vec_proj[TJ_ID]):\n",
    "    #    angles_raw = np.arctan2(triplet[:,0], triplet[:,1])\n",
    "    #    angles_zeroed[t] = angles_raw - angles_raw[0]\n",
    "     \n",
    "    # Rotate each triplet to lay the first vector onto zero angle [vectorized]\n",
    "    angles_raw = np.arctan2(TJs_vec_proj[TJ_ID][:,:,0], TJs_vec_proj[TJ_ID][:,:,1])\n",
    "    angles_zeroed = angles_raw - angles_raw[:, 0, np.newaxis]\n",
    "\n",
    "    ## Function: if flipped is better than the consensus (here the median), then flip [sequential]\n",
    "    #def flip_improvement(angles_zeroed):\n",
    "    #    median = np.median(angles_zeroed, axis=0)\n",
    "    #    for t in range(len(angles_zeroed)):\n",
    "    #        diff_original = np.abs(wrap_sub( angles_zeroed[t], median)) \n",
    "    #        diff_flipped  = np.abs(wrap_sub(-angles_zeroed[t], median))\n",
    "    #        if np.sum(diff_flipped) < np.sum(diff_original):\n",
    "    #            angles_zeroed[t] = - angles_zeroed[t]\n",
    "    #    return angles_zeroed\n",
    "    \n",
    "    # Function: if flipped is better than the consensus (here the median), then flip [vectorized]\n",
    "    def flip_improvement(angles_zeroed):\n",
    "        median = np.median(angles_zeroed, axis=0)\n",
    "        diff_original = np.abs(wrap_sub( angles_zeroed, median))\n",
    "        diff_flipped  = np.abs(wrap_sub(-angles_zeroed, median))\n",
    "        flip_mask = np.sum(diff_flipped, axis=1) < np.sum(diff_original, axis=1)\n",
    "        angles_zeroed[flip_mask] = -angles_zeroed[flip_mask]\n",
    "        return angles_zeroed\n",
    "    \n",
    "    # Run flip improvement until there is either...\n",
    "    # ...no change from one step to the next, or\n",
    "    # ...no improvement since 5 steps ago\n",
    "    median  = np.median(angles_zeroed, axis=0)\n",
    "    losses  = [np.abs(wrap_sub(angles_zeroed, median))]\n",
    "    counter = 0\n",
    "    while True:\n",
    "        \n",
    "        # Run a flip\n",
    "        angles_zeroed_new = flip_improvement(angles_zeroed)\n",
    "        \n",
    "        # Break if it changed nothing\n",
    "        if np.all(angles_zeroed==angles_zeroed_new):\n",
    "            break\n",
    "            \n",
    "        # Otherwise, compute and keep the new loss\n",
    "        median = np.median(angles_zeroed_new, axis=0)\n",
    "        losses.append(np.abs(wrap_sub(angles_zeroed_new, median)))\n",
    "        \n",
    "        # Break if the new loss is worse or equal to the loss 5 steps ago\n",
    "        if (counter >= 5) and losses[-1] >= losses[-6]:\n",
    "            break\n",
    "            \n",
    "        # Update\n",
    "        angles_zeroed = angles_zeroed_new\n",
    "        counter += 1\n",
    "    \n",
    "    # Convert back to unit vectors\n",
    "    for t in range(len(angles_zeroed)):\n",
    "        triplets_aligned[t] = circle(1.0, 0.0, 0.0, angles_zeroed[t]).T\n",
    "        \n",
    "    # Store results\n",
    "    TJs_vec_aligned[TJ_ID] = triplets_aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Show the resulting aligned vector triplets \n",
    "\n",
    "# For each TJ...\n",
    "cols = ['r','g','b']\n",
    "for TJ_ID in TJs_vec_aligned.keys():\n",
    "    \n",
    "    # Prep\n",
    "    plt.figure()\n",
    "    \n",
    "    # Plot each vec...\n",
    "    for vec in TJs_vec_aligned[TJ_ID]:\n",
    "        for i,v in enumerate(vec):\n",
    "            plt.plot([0,v[1]], [0,v[0]], c=cols[i])\n",
    "            \n",
    "    # Finalize\n",
    "    plt.title(str(TJ_ID))\n",
    "    plt.xlabel('v'); plt.ylabel('u')\n",
    "    plt.axis('equal')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Consensus Incident Vector Triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate a consensus incident vector triplet for each TJ\n",
    "\n",
    "# FLAG -- PRECISION, FLAG -- ROBUSTNESS: This is currently done in a very simple fashion.\n",
    "#                                        It probably works fine / doesn't matter much for\n",
    "#                                        data with a high z-resolution. However, there is\n",
    "#                                        room for improvement for low z-resolution data!\n",
    "\n",
    "# For each TJ...\n",
    "TJs_vec_consensus = {}\n",
    "for TJ_ID in TJs_vec_aligned.keys():\n",
    "    \n",
    "    # Prep result container\n",
    "    vecs_cons = np.empty((3,2))\n",
    "    \n",
    "    # Compute the mean of vectors within the 25-75th percentile\n",
    "    p25, p75 = np.percentile(TJs_vec_aligned[TJ_ID], [25, 75], axis=0)\n",
    "    for v in range(3):\n",
    "        vec  = TJs_vec_aligned[TJ_ID][:,v,:]\n",
    "        mask = (vec >= p25[v]) & (vec <= p75[v])\n",
    "        mean_v = np.mean(vec[mask[:,0],0])\n",
    "        mean_u = np.mean(vec[mask[:,1],1])\n",
    "        vecs_cons[v] = [mean_v, mean_u]\n",
    "        \n",
    "    # Renormalize\n",
    "    TJs_vec_consensus[TJ_ID] = (vecs_cons.T / np.sqrt(np.sum(vecs_cons**2.0, axis=1))).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Show the resulting consensus vector triplets \n",
    "\n",
    "# For each TJ...\n",
    "cols = ['r','g','b']\n",
    "for TJ_ID in TJs_vec_consensus.keys():\n",
    "    \n",
    "    # Prep\n",
    "    plt.figure()\n",
    "    \n",
    "    # Plot individual vecs...\n",
    "    for vec in TJs_vec_aligned[TJ_ID]:\n",
    "        for i,v in enumerate(vec):\n",
    "            plt.plot([0,v[1]], [0,v[0]], c=cols[i], alpha=0.1)\n",
    "            \n",
    "    # Plot consensus vecs\n",
    "    for i,v in enumerate(TJs_vec_consensus[TJ_ID]):\n",
    "        plt.plot([0,v[1]], [0,v[0]], c=cols[i], lw=4)\n",
    "            \n",
    "    # Finalize\n",
    "    plt.title(str(TJ_ID))\n",
    "    plt.xlabel('v'); plt.ylabel('u')\n",
    "    plt.axis('equal')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving the Force Balance Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prep: Assembling Equation Matrix G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Assemble G\n",
    "\n",
    "# Initialize zero matrix of shape (2 * num of TJs, num of DJs)\n",
    "G = np.zeros((2*len(TJs_vec_consensus), len(DJs)))\n",
    "\n",
    "# For each TJ...\n",
    "DJs_all_IDs = list(DJs.keys())\n",
    "for TJ_idx, TJ_ID in enumerate(TJs_vec_consensus.keys()):\n",
    "    \n",
    "    # Get all relevant DJs\n",
    "    DJ_IDs  = list(itertools.combinations(TJ_ID, 2))\n",
    "    \n",
    "    # For each DJ...\n",
    "    for DJ_ref, DJ_ID in enumerate(DJ_IDs):\n",
    "        \n",
    "        # Get index (in G) of the current DJ\n",
    "        DJ_idx = DJs_all_IDs.index(DJ_ID)\n",
    "\n",
    "        # Fill the appropriate positions in G\n",
    "        G[TJ_idx, DJ_idx] = TJs_vec_consensus[TJ_ID][DJ_ref][0]\n",
    "        G[len(TJs_vec_consensus)+TJ_idx, DJ_idx] = TJs_vec_consensus[TJ_ID][DJ_ref][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualize the result\n",
    "print(G.shape) # Should show G.shape[0] >= G.shape[1] (num of eqs >= num of interfaces)!\n",
    "plt.imshow(G)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solve using the `contraints` kwarg of scipy's minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define loss and constraints\n",
    "\n",
    "# Loss: sum of square deviations of equilibrium equations\n",
    "def eq_loss_c(gammas, G):\n",
    "    loss = (np.dot(G, gammas))**2.0\n",
    "    return np.sum(loss)\n",
    "\n",
    "# Constraint: the mean of tensions must be 1\n",
    "def eq_constraint(gammas):\n",
    "    c = np.mean(gammas) - 1\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run the fit\n",
    "fit = optimize.minimize(eq_loss_c, np.ones(len(DJs)), args=(G,), \n",
    "                        constraints={'type':'eq', 'fun':eq_constraint})\n",
    "tensions_c = fit.x\n",
    "DJs_tensions_c = {DJ_ID:tensions_c[DJ_num] for DJ_num, DJ_ID in enumerate(DJs_all_IDs)}\n",
    "print(tensions_c)\n",
    "\n",
    "# FLAG -- ISSUE: The softest interface has a negative tension. I'm not sure\n",
    "#                if that indicates a problem. It might be perfectly fine;\n",
    "#                after all, the tensions are relative to the mean and they\n",
    "#                are effective surface tensions, so high adhesion should\n",
    "#                be able to make them net-negative. Furthermore, this\n",
    "#                synthetic test sample has been constructed from geometric\n",
    "#                objects, so it doesn't represent a realistic structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Show tensions on image stack\n",
    "\n",
    "@interact(z=(0, im.shape[0]-1, 1))\n",
    "def show_stack(z=im.shape[0]//2):\n",
    "    \n",
    "    # Prep and plot image\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.imshow(outlines_id[z], cmap='gray')\n",
    "    \n",
    "    # For each DJ...\n",
    "    for DJ_num, DJ_ID in enumerate(DJs.keys()):\n",
    "        \n",
    "        # Get the DJ's DNs in the selected z plane\n",
    "        DNs_in_plane = DNIs[DJs[DJ_ID]][DNIs[DJs[DJ_ID]][:,0]==z]\n",
    "        \n",
    "        # Plot the points\n",
    "        plt.scatter(DNs_in_plane[:, 2], DNs_in_plane[:, 1],\n",
    "                    c=[tensions_c[DJ_num] for _ in range(DNs_in_plane.shape[0])],\n",
    "                    vmin=np.min(tensions_c), vmax=np.max(tensions_c),\n",
    "                    cmap='viridis', s=20)\n",
    "    \n",
    "    # Finish\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solve using Brodland et alii's Lagrange Multiplier Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prepare the matrix\n",
    "\n",
    "# Gdot\n",
    "Gdot = np.dot(G.T, G)\n",
    "\n",
    "# Show\n",
    "plt.imshow(Gdot)\n",
    "plt.show()\n",
    "\n",
    "# Add the constraints\n",
    "Gready = np.zeros((Gdot.shape[0]+1, Gdot.shape[1]+1))\n",
    "Gready[:Gdot.shape[0], :Gdot.shape[1]] = Gdot\n",
    "Gready[-1,:-1] = 1.0\n",
    "Gready[:-1,-1] = 1.0\n",
    "\n",
    "# Show\n",
    "plt.imshow(Gready)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define loss\n",
    "\n",
    "# Loss combining tension fit and constraint\n",
    "def eq_loss_l(gammas_lagrange, Gready):\n",
    "    loss = np.sum(np.dot(Gready[:-1], gammas_lagrange)**2.0)  # Fit loss\n",
    "    loss += (np.dot(Gready[-1], gammas_lagrange) - (gammas_lagrange.size-1))**2.0  # Constraint loss    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run the fit\n",
    "fit = optimize.minimize(eq_loss_l, np.ones(len(DJs)+1), args=(Gready,))\n",
    "tensions_l = fit.x[:-1]\n",
    "lagrange = fit.x[-1]\n",
    "DJs_tensions_l = {DJ_ID:tensions_l[DJ_num] for DJ_num, DJ_ID in enumerate(DJs_all_IDs)}\n",
    "print(tensions_l)\n",
    "print(lagrange)\n",
    "\n",
    "# FLAG -- ISSUE: Negative tension value, same as above with the scipy-based\n",
    "#                approach. See flag there for more info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Show tensions on image stack\n",
    "\n",
    "@interact(z=(0, im.shape[0]-1, 1))\n",
    "def show_stack(z=im.shape[0]//2):\n",
    "    \n",
    "    # Prep and plot image\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.imshow(outlines_id[z], cmap='gray')\n",
    "    \n",
    "    # For each DJ...\n",
    "    for DJ_num, DJ_ID in enumerate(DJs.keys()):\n",
    "        \n",
    "        # Get the DJ's DNs in the selected z plane\n",
    "        DNs_in_plane = DNIs[DJs[DJ_ID]][DNIs[DJs[DJ_ID]][:,0]==z]\n",
    "        \n",
    "        # Plot the points\n",
    "        plt.scatter(DNs_in_plane[:, 2], DNs_in_plane[:, 1],\n",
    "                    c=[tensions_l[DJ_num] for _ in range(DNs_in_plane.shape[0])],\n",
    "                    vmin=np.min(tensions_l), vmax=np.max(tensions_l),\n",
    "                    cmap='viridis', s=20)\n",
    "    \n",
    "    # Finish\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare Solver vs Lagrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot against each other\n",
    "\n",
    "# Prep\n",
    "plt.figure(figsize=(5,5))\n",
    "\n",
    "# Plot\n",
    "plt.scatter(tensions_l, tensions_c, s=50,\n",
    "            c='darkblue', lw=0.5, edgecolor='cyan')\n",
    "\n",
    "# Add equality line\n",
    "xlims, ylims = plt.gca().get_xlim(), plt.gca().get_ylim()\n",
    "plt.plot([-10,10], [-10,10], 'k-', zorder=-1, lw=1, alpha=0.5)\n",
    "plt.xlim(xlims); plt.ylim(ylims)\n",
    "\n",
    "# Labels\n",
    "plt.xlabel(\"inferred tension\\n[lagrange multiplier]\")\n",
    "plt.ylabel(\"inferred tension\\n[scipy constraint]\")\n",
    "\n",
    "# Finalize\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
