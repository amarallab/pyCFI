{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pyCFI Validation Notebook - Brodland Approach - Brodland 2017 Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "- General open issues are marked in comments using the word `FLAG`\n",
    "\n",
    "\n",
    "- Validation-specific open issues are marked in comments using the word `VALFLAG`\n",
    "\n",
    "\n",
    "- ***TO BE INVESTIGATED:***\n",
    "    - Why is there a case that loses 30pts during vector extraction? **[done]**\n",
    "        - Seems to be because it's an interface purely in a single z-plane (doesn't work for Brodland)\n",
    "    - Why are there cases where all three vectors are just in a line? **[done]**\n",
    "        - Also seems to be because it's an interface purely in a single z-plane (doesn't work for Brodland)\n",
    "        - Not sure why the problem this causes here is different to the one above...\n",
    "        - **Solution:** Exclude these problem cases from the very start!\n",
    "            - This leads to a much more clean set of TJs...\n",
    "            - ...but it also reduces the number of TJs quite substantially!\n",
    "            - Now there are only `2*10` equations for `27` interfaces; no good!\n",
    "    - See about also removing poor interfaces too, reducing the number of interfaces **[done]**\n",
    "        - Simply switching the initial selection to same-plane doesn't actually reduce the number\n",
    "        - This is because the interfaces are generally much bigger and stretch across multiple planes anyways\n",
    "        - Is there no way being a little less restrictive with the number of TJs removed...\n",
    "        - ...whilst still making sure the TJs kept are generally of decent quality?\n",
    "    - Does everything get better with a smoother segmentation? **[done]**\n",
    "        - Smoothing it with some dilation+erosion makes for a slight improvement\n",
    "        - It may be worth doing my own segmentation; their line enhancement filter is very suboptimal!\n",
    "    - Rather than removing the 3D component from the DN/TN search, remove TJs that don't go across `min_z` levels \n",
    "        - Something is wrong: a garbage TJ is being included!\n",
    "            - You forgot to double-check min_z also after the spline resampling and after the NaN exclusion!\n",
    "            - Also, you need to remove DJs, too! Otherwise they become the problem!\n",
    "        - With this fixed, the result no longer features crazy ridiculous outliers!\n",
    "        - This may be the most important bit to port to `Improved`\n",
    "    - *The result now no longer has carzy outliers but is still not really correlated with the Pipette data!*\n",
    "    - Maybe also add an absolute minimum size for DJs (based on some reasoning...) **[done]**\n",
    "    - Check the vectors; which ones are messy and why? *[done]*\n",
    "        - Issue 1: Incorrect angles/directionality due to non-adaptive query range *[fixed]*\n",
    "        - Issue 2: Incorrect handling of removed DJs due to defaultdict behavior *[fixed]*\n",
    "        - Issue 3: Sometimes there are too few/close-by neighbors for a TN so fitting fails *[fixed]*\n",
    "        - Issue 4: If the close-by neighbors are too \"blobby\" the fitting fails *[sort of fixed...]\n",
    "            - Added a way of removing these during arc fitting... but there are too many!\n",
    "            - There needs to be a way of having less crappy (broad) boundaries in the first place \n",
    "                - ~~Maybe the extracted outlines could be skeletonized~~ [doesn't work and is anyway questionable]\n",
    "                - No idea how to achieve this...\n",
    "            - \"Stupid\" solution: whenever there is an inscribed circle, just use the data's center of mass for the vector\n",
    "                - This is not very clean but it's an okay-ish fallback and also supercedes the straight-line exception\n",
    "    - Fix a bunch of issues related to angle wrapping (add `wrap_median` and use `wrap_sub` more often) **[done]**\n",
    "    - Check of how consistent a triplet consensus is; remove the ones that are poor **[done]**\n",
    "    - How else to reduce noise??? ***[YAH! - but first do the TODO in \"TO BE PORTED\" below!]***\n",
    "        - Consider re-segmenting yourself to get a less messy segmentation...\n",
    "        - Look through precision flags (and maybe robustness flags) for possible improvements\n",
    "        - Could smooth interpolation of additional slices help? ***[YAH!]***\n",
    "            - This might make the boundaries less \"broad\" and add more options for consensus!\n",
    "            - Implementing this by means of a \"majority filter\" turned out harder than expected\n",
    "            - The current solution based on `skimage.filters.rank.windowed_histogram` seems sensible, though\n",
    "                - **[yah]** Fix the problem with unexpectedly large values being returned sometimes!\n",
    "        \n",
    "        \n",
    "- ***TO BE PORTED:***\n",
    "    - Add the `min_DNs` constraint\n",
    "    - Correct `<= min_TNs` to `< min_TNs` where necessary\n",
    "    - Add `figsize=(4,4)` and `plt.{x|y}lim([-1.1, 1.1])` to all three vector plotting loops\n",
    "    - Switch reporting in NaN removal to `... \"remain in TJ\", TJ_ID ...` for better reference\n",
    "    - Add check for empty columns in `G`.\n",
    "    - Adjust the query range for finding TN-adjacent points during arc fitting to the resolution!\n",
    "    - Convert TJs and DJs defaultdicts to dicts and add appropriate handling\n",
    "    - Removal of TNs where only very few or only very closely bunched up neighbors are found during arc fitting\n",
    "    - Removal of TNs where the arc fitting erroneously produces an \"inscribed\" circle\n",
    "    - Improve the TJ spline visualization to keep the same colors as in the previous 3D plot\n",
    "    - Arc fitting: remove the overly specific straight line fix, replace by inscribed circle detection + centroid fix\n",
    "    - More angle wrapping issues:\n",
    "        - Copy over the `wrap_median` function\n",
    "        - Use it instead of `np.median` wherever appropriate\n",
    "        - Use `wrap_sub` for initial zeroing during triplet alignment \n",
    "    - Add comments to parameters/settins\n",
    "    - Add the new consensus calculation and the max deviation treshold\n",
    "    - Add dropping of columns from `G` (whilst handling relevant 'linked' objects)!\n",
    "    - Add `vmin` and `vmax` to all the `imshow`s of segmentations and outlines (to preserve seg ID colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports\n",
    "\n",
    "import itertools, collections\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "from skimage import io\n",
    "from scipy import spatial\n",
    "from scipy import interpolate\n",
    "from scipy import optimize\n",
    "import sympy as sym\n",
    "\n",
    "from ipywidgets import interact\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parameters & Settings\n",
    "\n",
    "fpath   = r'..\\Data\\Brodland\\brodland_20190726_clean\\t000004\\t000004_processed_segmented.tif'\n",
    "res     = np.array([1.0, 0.25, 0.25])  # Voxel sizes (z,y,x) in microns  # VALFLAG: ESTIMATED! ASK JIM VELDHUIS!\n",
    "se_size = 20   # VALFLAG: size of structural element for majority filter-based interpolation of z-slices\n",
    "min_TNs =   3  # Min TNs detected in the stack for a given TJ (TJ eliminated otherwise)\n",
    "min_DNs = 500  # Min DNs detected in the stack for a given DJ (DJ eliminated otherwise)\n",
    "min_z   =   3  # Min number of z slices a TJ or DJ must occur in (TJ or DJ eliminated otherwise)\n",
    "t_cons  =  10  # Max allowed mean vector deviation from consensus [in % of unit circle](TJ elimiated otherwise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load input segmentation stack\n",
    "\n",
    "im = io.imread(fpath) - 1 # VALFLAG: They had the background set to 1!\n",
    "print(im.dtype, im.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Show input stack\n",
    "\n",
    "@interact(z=(0, im.shape[0]-1, 1))\n",
    "def show_stack(z=im.shape[0]//2):\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.imshow(im[z], cmap='gray', vmin=im.min(), vmax=im.max())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### VALFLAG: Use morphological filters to smoothen the segmentation outlines \n",
    "\n",
    "from scipy.ndimage import grey_dilation, grey_erosion\n",
    "\n",
    "num_iters = 10\n",
    "im2 = np.copy(im)\n",
    "\n",
    "d = 9\n",
    "struct = (np.mgrid[:d,:d][0] - np.floor(d/2))**2 + (np.mgrid[:d,:d][1] - np.floor(d/2))**2 <= np.floor(d/2)**2\n",
    "struct3D = np.zeros((3, d, d), dtype=np.bool)\n",
    "struct3D[1, :, :] = struct\n",
    "\n",
    "for i in range(num_iters):\n",
    "    im2 = grey_dilation(im2, footprint=struct3D)\n",
    "    im2 = grey_erosion(im2, footprint=struct3D)\n",
    "    im2 = grey_erosion(im2, footprint=struct3D)\n",
    "    im2 = grey_dilation(im2, footprint=struct3D)\n",
    "\n",
    "im = im2\n",
    "print(im.dtype, im.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### VALFLAG: Show improved input stack\n",
    "\n",
    "@interact(z=(0, im.shape[0]-1, 1))\n",
    "def show_stack(z=im.shape[0]//2):\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.imshow(im[z], cmap='gray', vmin=im.min(), vmax=im.max())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### VALFLAG: Use interpolation to increase the number of z-slices [SLOW!]\n",
    "\n",
    "# VALFLAG: PERFORMANCE -- This can take a VERY long time to run (even for a stack that isn't huge)\n",
    "#                         but it only needs to be run once, then the result can be saved and used\n",
    "#                         in future. If and how to include this approach in the pyCFI distribution\n",
    "#                         version is an open question!\n",
    "\n",
    "from scipy.ndimage import generic_filter\n",
    "\n",
    "# Modal filter function fro generic_filter\n",
    "def modal_filt(footprint):\n",
    "    return np.argmax(np.bincount(footprint.astype(np.uint16)))\n",
    "\n",
    "# Interpolation function\n",
    "def seg_interp(im):\n",
    "    imi = np.zeros((im.shape[0]*2-1, im.shape[1], im.shape[2]))\n",
    "    for s in range(im.shape[0]-1):\n",
    "        imi[s*2+1] = generic_filter(im[s:s+2,:,:], modal_filt, size=se_size)[0]\n",
    "    imi[::2] = im\n",
    "    return imi\n",
    "\n",
    "im2 = seg_interp(im[20:25, 200:300, 200:300])  # Quite fast (good for testing)\n",
    "#im2 = seg_interp(im)  # Very slow(full data)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### VALFLAG: Use interpolation to increase the number of z-slices [parallelized with dask] [NON-FUNCTIONAL!]\n",
    "\n",
    "# VALFLAG: STABILITY -- Something is wrong either with the way I've implemented the dask\n",
    "#                       parallelization here or with its interaction with generic_filter\n",
    "#                       (maybe in conjunction with some memory issues). When run for a\n",
    "#                       small substack, it all works exactly as intended, but when run \n",
    "#                       for the full stack, python crashes (hard crash; no error!)\n",
    "# VALFLAG: PERFORMANCE -- This can take a long time to run (even for a stack that isn't huge)\n",
    "#                         but it only needs to be run once, then the result can be saved and\n",
    "#                         used in future. If and how to include this approach in the pyCFI\n",
    "#                         distribution version is an open question!\n",
    "\n",
    "# Get generic filter\n",
    "from scipy.ndimage import generic_filter\n",
    "\n",
    "# Get dask stuff\n",
    "from dask.distributed import Client, progress\n",
    "import dask.delayed as delayed\n",
    "import dask.bag as db\n",
    "\n",
    "# Start dask client\n",
    "Start dask client for parallelized z-interpolation\n",
    "client = Client(processes=False, memory_limit='4GB')\n",
    "display(client)\n",
    "\n",
    "# Modal filter function for generic_filter\n",
    "def modal_filt(footprint):\n",
    "    return np.argmax(np.bincount(footprint.astype(np.uint16)))\n",
    "\n",
    "# Wrapper for parallelization with dask\n",
    "def wrapper(s, im, function, size=None):\n",
    "    return generic_filter(im[s:s+2,:,:], function, size=size)[0]\n",
    "\n",
    "# Interpolation function\n",
    "def seg_interp(im):\n",
    "    \n",
    "    # Run the interpolation (parallelized with dask)\n",
    "    bag = db.from_sequence(range(im.shape[0]-1))\n",
    "    out = bag.map(wrapper, im, modal_filt, size=se_size).compute()\n",
    "    \n",
    "    # Assemble the resulting stack\n",
    "    imi = np.zeros((im.shape[0]*2-1, im.shape[1], im.shape[2]))\n",
    "    for s in range(im.shape[0]-1):\n",
    "        imi[s*2+1] = out[s]\n",
    "    imi[::2] = im\n",
    "    \n",
    "    # Done\n",
    "    return imi\n",
    "\n",
    "im2 = seg_interp(im[20:25, 200:300, 200:300])  # Works!\n",
    "#im2 = seg_interp(im)  # Crashes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### VALFLAG: Use interpolation to increase the number of z-slices [SLOW!]\n",
    "\n",
    "from skimage.filters.rank import windowed_histogram\n",
    "\n",
    "# Get number of bins to use\n",
    "n_bins = im.max() + 1\n",
    "\n",
    "# Interpolation function\n",
    "def seg_interp(im):\n",
    "    \n",
    "    imi = np.zeros((im.shape[0]*2-1, im.shape[1], im.shape[2]))\n",
    "    \n",
    "    for s in range(im.shape[0]-1):\n",
    "        \n",
    "        s_hist  = windowed_histogram(im[s,:,:],   np.ones((se_size,se_size)), n_bins=n_bins)\n",
    "        s_hist += windowed_histogram(im[s+1,:,:], np.ones((se_size,se_size)), n_bins=n_bins)\n",
    "        \n",
    "        # VALFLAG: STABILITY -- What causes these weird edge cases?\n",
    "        print(s_hist.sum())\n",
    "        if (s_hist.sum() != 2.0*im[0].size) and (s_hist.sum() != 0.0):\n",
    "            s_hist = np.zeros((im[0].shape[0], im[0].shape[1], n_bins))  # VALFLAG: YAH -- THIS IS NOT A SOLUTION!\n",
    "        \n",
    "        imi[s*2+1] = np.argmax(s_hist, axis=-1)\n",
    "        \n",
    "    imi[::2] = im\n",
    "    \n",
    "    return imi\n",
    "\n",
    "im2 = seg_interp(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### VALFLAG: Show improved input stack\n",
    "\n",
    "@interact(z=(0, im2.shape[0]-1, 1))\n",
    "def show_stack(z=im2.shape[0]//2):\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.imshow(im2[z], cmap='gray', vmin=im2.min(), vmax=im2.max())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying Object Outlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Identify outline voxels by comparing shifted images\n",
    "\n",
    "# Pad the image by 1 voxel on all sides\n",
    "im_pad = np.pad(im, 1, mode='reflect')\n",
    "\n",
    "# Get possible shifts in all directions\n",
    "shifts = itertools.product([0,1], repeat=3)\n",
    "\n",
    "# Check and accumulate differences in shifts\n",
    "outlines = np.zeros_like(im, dtype=np.bool)\n",
    "for shift in shifts:\n",
    "    zs0, ys0, xs0 = [slice(1, None) if s else slice(None) for s in shift]\n",
    "    zs1, ys1, xs1 = [slice(None,-1) if s else slice(None) for s in shift]\n",
    "    comparison = im_pad[zs0, ys0, xs0] != im_pad[zs1, ys1, xs1]\n",
    "    outlines  += comparison[:im.shape[0],  :im.shape[1],  :im.shape[2]]\n",
    "    outlines  += comparison[-im.shape[0]:, -im.shape[1]:, -im.shape[2]:]  # Symmetry\n",
    "    \n",
    "# Re-annotate the cell identities\n",
    "outlines_id = outlines * im\n",
    "\n",
    "# Report\n",
    "print(outlines.dtype, outlines.shape)\n",
    "print(outlines_id.dtype, outlines_id.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Show identified outlines\n",
    "\n",
    "@interact(z=(0, im.shape[0]-1, 1))\n",
    "def show_stack(z=im.shape[0]//2):\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.imshow(outlines_id[z], cmap='gray',\n",
    "               vmin=outlines_id.min(), vmax=outlines_id.max())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying Triple Nodes (TNs) and Triple Junctions (TJs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Find coordinates of all voxels involved in triple nodes\n",
    "\n",
    "# FLAG: PERFORMANCE -- This could potentially be done with image shifting much like `outlines` above!\n",
    "# FLAG: PRECISION -- For the coordinates, would a `+1.0` be more appropriate at interfaces between two cells?\n",
    "\n",
    "# NOTE: Specifically for the Brodland approach, only those TNs are identified that\n",
    "#       have all three interacting cells in the same plane!\n",
    "\n",
    "# Get Outline Indices (OIs) and Outline Coordinates (OCs)\n",
    "OIs = np.array(np.where(outlines)).T\n",
    "OCs = (OIs + 0.5) * res\n",
    "\n",
    "# Go through OIs and find TN Indices (TNIs)\n",
    "TNIs = []\n",
    "for OI in OIs:\n",
    "    selection = im_pad[OI[0]+1:OI[0]+3, OI[1]+1:OI[1]+3, OI[2]+1:OI[2]+3]\n",
    "    if len(set(selection.flatten())) == 3:\n",
    "        TNIs.append(OI)\n",
    "TNIs = np.array(TNIs)\n",
    "\n",
    "# Convert to TN Coordinates (TNCs)\n",
    "TNCs = (TNIs + 0.5) * res\n",
    "\n",
    "# Report\n",
    "print('OCs: ', OCs.shape)\n",
    "print('TNIs:', TNIs.shape)\n",
    "print('TNCs:', TNCs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Build a dict of TJs structured as: {tuple(cell1_ID, cell2_ID, cell3_ID) : array(INDICES INTO TNIs/TNCs)}\n",
    "\n",
    "# NOTE: Specifically for the Brodland approach, only those TNs are identified that\n",
    "#       have all three interacting cells in the same plane!\n",
    "\n",
    "# Prepare defaultdict\n",
    "TJs = collections.defaultdict(lambda : [])\n",
    "\n",
    "# Go through TNs, create IDs, assign coordinates to IDs\n",
    "for idx,TNI in enumerate(TNIs):\n",
    "    selection = im_pad[np.int(TNI[0])+1:np.int(TNI[0])+3, \n",
    "                       np.int(TNI[1])+1:np.int(TNI[1])+3, \n",
    "                       np.int(TNI[2])+1:np.int(TNI[2])+3]\n",
    "    TJ_ID = tuple(sorted(set(selection.flatten())))\n",
    "    TJs[TJ_ID].append(idx)\n",
    "    \n",
    "# Convert TJ lists to numpy arrays & remove unwanted\n",
    "for TJ_ID in list(TJs.keys()):\n",
    "    \n",
    "    # Remove if too short\n",
    "    if len(TJs[TJ_ID]) < min_TNs:\n",
    "        del TJs[TJ_ID]\n",
    "        continue\n",
    "        \n",
    "    # Remove if not across >min_z z-slices\n",
    "    if np.unique(TNIs[TJs[TJ_ID]][:,0]).size < min_z:\n",
    "        del TJs[TJ_ID]\n",
    "        continue\n",
    "    \n",
    "    # Convert to array\n",
    "    TJs[TJ_ID] = np.array(TJs[TJ_ID])\n",
    "\n",
    "# Convert to non-defaultdict to avoid unexpected behavior\n",
    "TJs = dict(TJs)\n",
    "    \n",
    "# Report\n",
    "print('TJs:', len(TJs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Show identified TJs on image stack\n",
    "\n",
    "@interact(z=(0, im.shape[0]-1, 1))\n",
    "def show_stack(z=im.shape[0]//2):\n",
    "    \n",
    "    # Prep and plot image\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.imshow(outlines_id[z], cmap='gray',\n",
    "               vmin=outlines_id.min(), vmax=outlines_id.max())\n",
    "    \n",
    "    # For each TJ...\n",
    "    for TJ_num,TJ_ID in enumerate(TJs.keys()):\n",
    "        \n",
    "        # Get the TJ's TNs in the selected z plane\n",
    "        TNs_in_plane = TNIs[TJs[TJ_ID]][TNIs[TJs[TJ_ID]][:,0]==z]\n",
    "        \n",
    "        # Plot the points\n",
    "        plt.scatter(TNs_in_plane[:, 2], TNs_in_plane[:, 1],\n",
    "                    c=[TJ_num for _ in range(TNs_in_plane.shape[0])], # Coloring trick!\n",
    "                    cmap='hsv', vmin=0, vmax=len(TJs), s=20)\n",
    "        \n",
    "    # Finish\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Show identified TJs as 3D scatter\n",
    "\n",
    "# Prepare the plot\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "ax  = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot each TJ in a different color\n",
    "for TJ_num,TJ_ID in enumerate(TJs.keys()):\n",
    "    ax.scatter(TNCs[TJs[TJ_ID]][:,2], TNCs[TJs[TJ_ID]][:,1], TNCs[TJs[TJ_ID]][:,0],\n",
    "               c=[TJ_num for _ in range(TJs[TJ_ID].shape[0])], \n",
    "               cmap='hsv', vmin=0, vmax=len(TJs), s=10)\n",
    "\n",
    "## Also show cell outlines [may take several seconds to render!]\n",
    "#ax.scatter([c[2] for c in OCs],\n",
    "#           [c[1] for c in OCs],\n",
    "#           [c[0] for c in OCs],\n",
    "#           c='gray', alpha=0.01, linewidth=0, s=5)\n",
    "\n",
    "## Axis limits\n",
    "#ax.set_xlim([0,200])\n",
    "#ax.set_ylim([0,200])\n",
    "#ax.set_zlim([0,200])\n",
    "\n",
    "# Finish\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### As an aside: also get the z-coordinates of imaging planes in real space\n",
    "\n",
    "image_z_coords = (np.arange(0, im.shape[0]) + 0.5) * res[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying Double Nodes (DNs) and Double Junctions (DJs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Find coordinates of all voxels involved in DOUBLE nodes\n",
    "\n",
    "# FLAG: PERFORMANCE -- Same as for TNI/TNC extraction above!\n",
    "# FLAG: PRECISION -- Same as for TNI/TNC extraction above!\n",
    "\n",
    "# NOTE: Specifically for the Brodland approach, only those DNs are identified that\n",
    "#       have all both interacting cells in the same plane!\n",
    "\n",
    "# Go through OIs and find DN Indices (DNIs)\n",
    "DNIs = []\n",
    "for OI in OIs:\n",
    "    selection = im_pad[OI[0]+1:OI[0]+3, OI[1]+1:OI[1]+3, OI[2]+1:OI[2]+3]\n",
    "    if len(set(selection.flatten())) == 2:\n",
    "        DNIs.append(OI)\n",
    "DNIs = np.array(DNIs)\n",
    "\n",
    "# Convert to DN Coordinates (DNCs)\n",
    "DNCs = (DNIs + 0.5) * res\n",
    "\n",
    "# Report\n",
    "print('OCs: ', OCs.shape)\n",
    "print('DNIs:', DNIs.shape)\n",
    "print('DNCs:', DNCs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Build a dict of Double Junctions (DJs) structured as: {tuple(cell1_ID, cell2_ID) : array(INDICES INTO DNIs/DNCs)}\n",
    "\n",
    "# NOTE: Specifically for the Brodland approach, only those DNs are identified that\n",
    "#       have all both interacting cells in the same plane!\n",
    "\n",
    "# Prepare defaultdict\n",
    "DJs = collections.defaultdict(lambda : [])\n",
    "\n",
    "# Go through DNs, create IDs, assign coordinates to IDs\n",
    "for idx,DNI in enumerate(DNIs):\n",
    "    selection = im_pad[np.int(DNI[0])+1:np.int(DNI[0])+3, \n",
    "                       np.int(DNI[1])+1:np.int(DNI[1])+3, \n",
    "                       np.int(DNI[2])+1:np.int(DNI[2])+3]\n",
    "    DJ_ID = tuple(sorted(set(selection.flatten())))\n",
    "    DJs[DJ_ID].append(idx)\n",
    "    \n",
    "# Convert DJ lists to numpy arrays & remove unwanted\n",
    "for DJ_ID in list(DJs.keys()):\n",
    "    \n",
    "    # Remove if too small\n",
    "    if len(DJs[DJ_ID]) <= min_DNs:\n",
    "        del DJs[DJ_ID]\n",
    "        continue\n",
    "    \n",
    "    # Remove if not across >min_z z-slices\n",
    "    if np.unique(DNIs[DJs[DJ_ID]][:,0]).size < min_z:\n",
    "        del DJs[DJ_ID]\n",
    "        continue\n",
    "    \n",
    "    # Convert to array\n",
    "    DJs[DJ_ID] = np.array(DJs[DJ_ID])\n",
    "    \n",
    "# Convert to non-defaultdict to avoid unexpected behavior\n",
    "DJs = dict(DJs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Show identified DJs on image stack\n",
    "\n",
    "@interact(z=(0, im.shape[0]-1, 1))\n",
    "def show_stack(z=im.shape[0]//2):\n",
    "    \n",
    "    # Prep and plot image\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.imshow(outlines_id[z], cmap='gray',\n",
    "               vmin=outlines_id.min(), vmax=outlines_id.max())\n",
    "    \n",
    "    # For each DJ...\n",
    "    for DJ_num,DJ_ID in enumerate(DJs.keys()):\n",
    "        \n",
    "        # Get the DJ's DNs in the selected z plane\n",
    "        DNs_in_plane = DNIs[DJs[DJ_ID]][DNIs[DJs[DJ_ID]][:,0]==z]\n",
    "        \n",
    "        # Plot the points\n",
    "        plt.scatter(DNs_in_plane[:, 2], DNs_in_plane[:, 1],\n",
    "                    c=[DJ_num for _ in range(DNs_in_plane.shape[0])], # Coloring trick!\n",
    "                    cmap='autumn', vmin=0, vmax=len(DJs), s=5, lw=0, alpha=0.5)\n",
    "        \n",
    "    # Finish\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting Splines to TJs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns out spline fitting requires the input points to be roughly in order along the spline, which isn't guaranteed in our case. Ordering the points happens to be far harder problem than one might imagine (it's a variation of traveling salesman) but luckily it can be solved quite well with a Breadth-First Search (BFS). This solution is partially inspired by Imanol Luengo's answer to [this SO question](https://stackoverflow.com/questions/37742358/sorting-points-to-form-a-continuous-line).\n",
    "\n",
    "<font color=orange>**Warning 1:**</font> This will fail for geometries that exhibit \"crossings\" or \"forks\" of any kind. Although that should be very rare/non-existent in the data, a special form of \"fork\" is the circle. In case of a fully circular TJ, which occurs when two cells neatly touch each other, this will fail (unless some points are removed from the TJ). I couldn't come up with a way of fixing this but divised the `InvalidPathError` to at least pick up on such cases. However, **it may be too stringent** as it is currently implemented!\n",
    "\n",
    "<font color=orange>**Warning 2:**</font> Simply rescaling the z axis a little bit already led to renewed problems with this approach, so I'm starting to seriously doubt its robustness. We'll have to keep a close eye on this and possibly somehow develop a better solution if problems keep cropping up. Maybe some sort of modified graph search (rather than straight up BFS) would be a possibility..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function to reorder TEs along the progression of the TJ\n",
    "\n",
    "# FLAG: ROBUSTNESS -- I still have my doubts as to the robustness of this approach (see warnings above)!\n",
    "#                     I keep wondering if there isn't a better way!\n",
    "\n",
    "# Define helpful custom exceptions\n",
    "class InvalidGraphError(Exception): pass\n",
    "class InvalidPathError(Exception): pass\n",
    "\n",
    "# Define function\n",
    "def sort_line_coords(coords, N_neighbors=10, source=None, \n",
    "                     return_argsort=False, ignore_path_check=False):\n",
    "    \"\"\"Given a set of coordinates that roughly lie on a 1D curve in mD space\n",
    "    (but may be in random order), sort the points such that they roughly follow \n",
    "    the curve's progression.\n",
    "    \n",
    "    Uses a breadth-first search tree on a nearest-neighbor graph of the coords,\n",
    "    which happens to result in the best possible sort. Does not work as intended\n",
    "    for closed curves and curves that form any kind of fork or crossing; an \n",
    "    Error is raised in such cases.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    coords : array of shape (N_points, M_dimensions)\n",
    "        Coordinates of points roughly lying on a point in M-dimensional space.\n",
    "    N_neighbors : int, optional, default 10\n",
    "        Number of nearest neighbors to include for each graph. If this is set\n",
    "        too low, connected components may form and no complete solution is\n",
    "        possible (raises an Exception). If this is set too high, the resulting\n",
    "        sort is very imprecises. The ideal value must be determined empirically.\n",
    "        When used to prepare TJs for spline fitting in the context of pyCFI, the\n",
    "        default (10) is a reasonably choice and the outcome is largely robust\n",
    "        to changes between values of 5 and 20.\n",
    "    source : None or int, optional, default None\n",
    "        The source is a point at one of the two ends of the line. If None, the\n",
    "        point is automatically determined by testing all different points and \n",
    "        selecting the one that yields the best sort (by minimizing the resulting\n",
    "        path distance). If source is an int, it indexes into coords to specify\n",
    "        the end point from which the sort is constructed. This saves a lot of\n",
    "        time compared to the automated search, especially if there are many\n",
    "        points, however it requires prior knowledge of the end point.\n",
    "    return_argsort : bool, optional, default False\n",
    "        If True, the index array that sorts the points into the best order is \n",
    "        returned as a second result. Otherwise, only a sorted version of coords \n",
    "        is returned.\n",
    "    ignore_path_check : bool, optional, default False\n",
    "        If True, the final path is not cross-checked and no InvalidPathErrors\n",
    "        can be raised (see Exceptions below).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    sorted_coords : array of shape (N_points, M_dimensions)\n",
    "        The same set of points as in the input coords but sorted along the\n",
    "        curve's progression in space.\n",
    "    best_path : array of shape (N_points,)\n",
    "        Index array that sorts points along the curve's progression in space. \n",
    "        Only returned if return_argsort is set to True.\n",
    "        \n",
    "    Exceptions\n",
    "    ----------\n",
    "    InvalidGraphError : If the adjacency graph created based on the kdTree is\n",
    "        not fully connected, InvalidGraphError is raised. This may imply that\n",
    "        N_neighbors is too low or that the points in coords do not belong to\n",
    "        a single continuous line.\n",
    "    InvalidPathError : If the curve is closed or contains forks/crossings, the\n",
    "        sort fails, which is reflected in the fact that the final path will\n",
    "        contain steps that do not have corresponding edges on the graph. In\n",
    "        this case, InvalidPathError is raised. This may also occur under other\n",
    "        dubious circumstances, e.g. if the input data is not a curve at all \n",
    "        or if it is a very broad curve or if N_neighbors is too low.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get pairwise distances (if needed)\n",
    "    if source is None:\n",
    "        dists = spatial.distance.squareform(spatial.distance.pdist(coords))\n",
    "    \n",
    "    # Get nearest neighbors\n",
    "    kdtree  = spatial.cKDTree(coords)\n",
    "    _, KNNs = kdtree.query(coords, k=N_neighbors if N_neighbors<coords.shape[0] else coords.shape[0])\n",
    "    \n",
    "    # Build adjacency matrix\n",
    "    adj_M = np.zeros((coords.shape[0], coords.shape[0]), dtype=np.bool)\n",
    "    for i,N in enumerate(KNNs):\n",
    "        adj_M[i,N] = True\n",
    "    \n",
    "    # Construct networkx graph\n",
    "    G = nx.from_numpy_array(adj_M)\n",
    "    if not nx.is_connected(G):\n",
    "        #class InvalidGraphError(Exception): pass\n",
    "        raise InvalidGraphError('Adjacency graph is not fully connected!')\n",
    "     \n",
    "    # If a source node is given, just get its BFS tree\n",
    "    if source is not None:\n",
    "        best_path = list(nx.bfs_tree(G, source))\n",
    "        \n",
    "    # Otherwise, find the best BFS tree from all sources\n",
    "    if source is None:\n",
    "        paths = []\n",
    "        costs = []\n",
    "        for n in G.nodes():\n",
    "\n",
    "            # Get BFS tree\n",
    "            path = list(nx.bfs_tree(G, n))\n",
    "\n",
    "            # Get sum of all distances within tree\n",
    "            cost = 0.0\n",
    "            for n0,n1 in zip(path, path[1:]):\n",
    "                cost += dists[n0, n1]\n",
    "\n",
    "            # Keep results\n",
    "            paths.append(path)\n",
    "            costs.append(cost)\n",
    "\n",
    "        # Select the best solution\n",
    "        best_path = paths[np.argmin(costs)]\n",
    "    \n",
    "    # Test for cases that probably failed\n",
    "    if not ignore_path_check:\n",
    "        for p1,p2 in zip(best_path, best_path[1:]):\n",
    "            if not G.has_edge(p1,p2):\n",
    "                raise InvalidPathError(\"The sort path uses an edge that is not on the graph. \"+\n",
    "                                       \"This should not happen and probably implies that the \"+\n",
    "                                       \"curve is cyclical or has a fork/crossing.\")\n",
    "    \n",
    "    # Sort coords and return\n",
    "    if return_argsort:\n",
    "        return coords[best_path], best_path\n",
    "    else:\n",
    "        return coords[best_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Wrapper for spline fitting\n",
    "\n",
    "def wrap_splprep(coords, k=3, verbose=False):\n",
    "    \"\"\"Fit an nD spline with scipy.interpolate.splprep.\n",
    "    \n",
    "    coords : array (points, dimensions) : input data\n",
    "    k=3 : integer : degrees of freedom\n",
    "    verbose=False : bool : wether to print all outputs\n",
    "    \n",
    "    returns -> tck : tuple (knots, coefficients, k) : \n",
    "               fit parameters as used by splev\n",
    "    \"\"\"\n",
    "    \n",
    "    # Fit the spline and unpack the (weirdly packaged) results\n",
    "    tcku, fp, ier, msg = interpolate.splprep(coords.T, k=k, full_output=True)\n",
    "    tck, u = tcku\n",
    "\n",
    "    # Report the results\n",
    "    if verbose:\n",
    "        print ('\\nt (knots, tck[0]):\\n' , tck[0])\n",
    "        print ('\\nc (coefficients, tck[1]):\\n' , tck[1])\n",
    "        print ('\\nk (degree, tck[2]):' , tck[2])\n",
    "        print ('\\nu (evaluation points):\\n', u)\n",
    "        print ('\\nfp (residual error):', fp)\n",
    "        print ('\\nier (error code; success is ier<=0):', ier)\n",
    "        print ('\\nmsg (message from FITPACK):\\n', msg)\n",
    "        \n",
    "    # Raise an error if FITPACK indicates failure\n",
    "    if ier > 0:\n",
    "        raise Exception('ier is >0, indicating that FITPACK failed somehow. '+\n",
    "                        'The message from FITPACK was:\\n'+msg)\n",
    "        \n",
    "    # Return the only result relevant to spline evaluation\n",
    "    return tck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Perform sorting and spline fitting on all TJs\n",
    "\n",
    "# FLAG -- PRECISION: Currently, cases where the TNs of a single TJ_ID do not form a single\n",
    "#                    continuous line are caught and those TJs are removed entirely (see\n",
    "#                    InvalidGraphError handling). However, such cases can naturally occur\n",
    "#                    in some (rare-ish) geometries involving 4+ cells and the background.\n",
    "#                    Would be nice to somehow recognize these cases and handle them better,\n",
    "#                    though the way the TJ_IDs are currently done wouldn't readily allow\n",
    "#                    such a solution...\n",
    "\n",
    "# Parameters\n",
    "num_ts = 1000   # Must be much larger than the number of z-slices crossed by any single TJ!\n",
    "tng_dv = 10e-2  # FLAG -- PRECISION: Should this be smaller? FLAG -- ROBUSTNESS: Should this scale with res?\n",
    "\n",
    "# Output dicts\n",
    "TJs_spline_tck     = {}  # Fitted splines for each TJ\n",
    "TJs_spline_t       = {}  # Parameter (t) values for evaluation\n",
    "TJs_spline_ev      = {}  # Evaluated splines (at each t) for each TJ\n",
    "TJs_spline_tangent = {}  # Tangents to splines for each TJ\n",
    "\n",
    "# For each TJ...\n",
    "for TJ_ID in list(TJs.keys()):\n",
    "    \n",
    "    # Sort coordinates along the line\n",
    "    try:\n",
    "        sorted_TJCs, TJ_argsort = sort_line_coords(TNCs[TJs[TJ_ID]],\n",
    "                                                   return_argsort=True,\n",
    "                                                   ignore_path_check=True)\n",
    "    except InvalidGraphError:  # Remove cases where points with the same TJ...\n",
    "        del TJs[TJ_ID]         # ...identifier don't form a continuous line.\n",
    "        continue\n",
    "    TJs[TJ_ID] = TJs[TJ_ID][TJ_argsort]\n",
    "    \n",
    "    # Perform spline fitting\n",
    "    tck = wrap_splprep(sorted_TJCs)\n",
    "    TJs_spline_tck[TJ_ID] = tck\n",
    "    \n",
    "    # Evaluate the spline in 1000 regular intervals\n",
    "    TJs_spline_t[TJ_ID] = np.linspace(0.0, 1.0, num_ts)\n",
    "    ev = interpolate.splev(TJs_spline_t[TJ_ID], tck)\n",
    "    ev = np.array(ev).T\n",
    "    TJs_spline_ev[TJ_ID] = ev\n",
    "    \n",
    "    # Also evaluate with slight deviation forward and backward\n",
    "    evD1 = np.array(interpolate.splev(TJs_spline_t[TJ_ID]+tng_dv, tck)).T\n",
    "    evD2 = np.array(interpolate.splev(TJs_spline_t[TJ_ID]-tng_dv, tck)).T\n",
    "    \n",
    "    # Approximate the tangent vector as the sum of the deviatory vectors\n",
    "    tangent_vec = ((evD1 - ev) + (ev - evD2)) / 2.0\n",
    "    TJs_spline_tangent[TJ_ID] = tangent_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Visualize the fitted splines and the tangent vectors as 3D scatter\n",
    "\n",
    "# Prepare the plot\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "ax  = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "## Plot each TJ in a different color\n",
    "#for TJ_num,TJ_ID in enumerate(TJs.keys()):\n",
    "#    ax.scatter(TNCs[TJs[TJ_ID]][:,2], TNCs[TJs[TJ_ID]][:,1], TNCs[TJs[TJ_ID]][:,0],\n",
    "#               c=[TJ_num for _ in range(TJs[TJ_ID].shape[0])], \n",
    "#               cmap='hsv', vmin=0, vmax=len(TJs), s=10)\n",
    "\n",
    "# Plot each TJ spline\n",
    "for TJ_num,TJ_ID in enumerate(TJs_spline_ev.keys()):\n",
    "    ax.scatter(TJs_spline_ev[TJ_ID][:,2], \n",
    "               TJs_spline_ev[TJ_ID][:,1], \n",
    "               TJs_spline_ev[TJ_ID][:,0],\n",
    "               c=[TJ_num for _ in range(TJs_spline_ev[TJ_ID].shape[0])],\n",
    "               cmap='hsv', vmin=0, vmax=len(TJs_spline_ev), s=5)\n",
    "\n",
    "# Add the tangent vectors\n",
    "for TJ_num, TJ_ID in enumerate(TJs_spline_ev.keys()):\n",
    "    for splpt, tngvec in zip(TJs_spline_ev[TJ_ID][::40], TJs_spline_tangent[TJ_ID][::40]):\n",
    "        plt.plot([splpt[2], splpt[2]+tngvec[2]],\n",
    "                 [splpt[1], splpt[1]+tngvec[1]],\n",
    "                 [splpt[0], splpt[0]+tngvec[0]],\n",
    "                 'r-', alpha=0.5)\n",
    "           \n",
    "## Axis limits\n",
    "#ax.set_xlim([0,200])\n",
    "#ax.set_ylim([0,200])\n",
    "#ax.set_zlim([0,200])\n",
    "\n",
    "# Finish\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get spline intersections with imaging planes\n",
    "\n",
    "# Prep result dicts\n",
    "TJs_itsc_t = {}\n",
    "TJs_itsc_ev = {}\n",
    "TJs_itsc_tangent = {}\n",
    "\n",
    "# For each TJ...\n",
    "for TJ_ID in list(TJs_spline_ev.keys()):\n",
    "    \n",
    "    # Get relevant spline points\n",
    "    ev = TJs_spline_ev[TJ_ID]\n",
    "\n",
    "    # Go through points...\n",
    "    intersect_ts = []\n",
    "    intersect_zs = []\n",
    "    for i in range(1, ev.shape[0]-1):\n",
    "        \n",
    "        # Find the closest imaging z-plane (in real space)\n",
    "        close_plane_z = image_z_coords[np.argmin(np.abs(image_z_coords - ev[i][0]))]\n",
    "        \n",
    "        # If the point crosses crossed that plane...\n",
    "        if ( ( (ev[i][0] < close_plane_z) and (ev[i-1][0] > close_plane_z) ) or\n",
    "             ( (ev[i][0] > close_plane_z) and (ev[i-1][0] < close_plane_z) ) ):\n",
    "            \n",
    "            # Linearly interpolate the spline t at that z-plane\n",
    "            dist_a = np.abs(ev[i-1][0] - close_plane_z)\n",
    "            dist_b = np.abs(ev[i][0] - close_plane_z)\n",
    "            dist_tot = dist_a + dist_b\n",
    "            dist_a = 1.0 - (dist_a / dist_tot)\n",
    "            dist_b = 1.0 - (dist_b / dist_tot)\n",
    "            t_a = TJs_spline_t[TJ_ID][i-1]\n",
    "            t_b = TJs_spline_t[TJ_ID][i]\n",
    "            t_interp = (dist_a * t_a) + (dist_b * t_b)\n",
    "            \n",
    "            # Keep the results\n",
    "            intersect_ts.append(t_interp)\n",
    "            intersect_zs.append(close_plane_z)\n",
    "    \n",
    "    # Convert to arrays\n",
    "    intersect_ts = np.array(intersect_ts)\n",
    "    intersect_zs = np.array(intersect_zs)\n",
    "    \n",
    "    # Drop TJs that no longer fulfill length/z-span requirements\n",
    "    if len(intersect_zs) < min_TNs or len(np.unique(intersect_zs)) < min_z:\n",
    "        del TJs[TJ_ID]\n",
    "        del TJs_spline_ev[TJ_ID]\n",
    "        del TJs_spline_t[TJ_ID]\n",
    "        del TJs_spline_tangent[TJ_ID]\n",
    "        del TJs_spline_tck[TJ_ID]\n",
    "        continue\n",
    "    \n",
    "    # Evaluate the spline at the intersecting t-values\n",
    "    intersect_ev = np.array(interpolate.splev(intersect_ts, TJs_spline_tck[TJ_ID])).T\n",
    "    intersect_ev[:, 0] = intersect_zs  # Ensure exact intersect z-values\n",
    "    \n",
    "    # Also evaluate with slight deviation forward and backward\n",
    "    intersect_evD1 = np.array(interpolate.splev(intersect_ts+tng_dv, TJs_spline_tck[TJ_ID])).T\n",
    "    intersect_evD2 = np.array(interpolate.splev(intersect_ts-tng_dv, TJs_spline_tck[TJ_ID])).T\n",
    "    \n",
    "    # Approximate the tangent vector as the sum of the deviatory vectors\n",
    "    intersect_tangent_vec = ((intersect_evD1 - intersect_ev) + (intersect_ev - intersect_evD2)) / 2.0\n",
    "    \n",
    "    # Keep all results\n",
    "    TJs_itsc_t[TJ_ID] = intersect_ts\n",
    "    TJs_itsc_ev[TJ_ID] = intersect_ev\n",
    "    TJs_itsc_tangent[TJ_ID] = intersect_tangent_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualize the spline points at z-plane intersections with their tangent vectors\n",
    "\n",
    "# Prepare the plot\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "ax  = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot each TJ spline\n",
    "for TJ_num,TJ_ID in enumerate(TJs_itsc_ev.keys()):\n",
    "    ax.scatter(TJs_itsc_ev[TJ_ID][:,2], \n",
    "               TJs_itsc_ev[TJ_ID][:,1], \n",
    "               TJs_itsc_ev[TJ_ID][:,0])\n",
    "\n",
    "# Add the tangent vectors\n",
    "for TJ_num, TJ_ID in enumerate(TJs_itsc_ev.keys()):\n",
    "    for splpt, tngvec in zip(TJs_itsc_ev[TJ_ID], TJs_itsc_tangent[TJ_ID]):\n",
    "        plt.plot([splpt[2], splpt[2]+tngvec[2]],\n",
    "                 [splpt[1], splpt[1]+tngvec[1]],\n",
    "                 [splpt[0], splpt[0]+tngvec[0]],\n",
    "                 'r-', alpha=0.5)\n",
    "            \n",
    "## Axis limits\n",
    "#ax.set_xlim([0,200])\n",
    "#ax.set_ylim([0,200])\n",
    "#ax.set_zlim([0,200])\n",
    "\n",
    "# Finish\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving Incident Vectors in the Sectioning Plane\n",
    "\n",
    "**Note:** The arc fitting approach taken here is based on the second approach described in [this scipy cookbook entry](https://scipy-cookbook.readthedocs.io/items/Least_Squares_Circle.html). It could probably be further improved by using the third approach, i.e. by explicitly specifying the Jacobian function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Functions for circular arc fitting\n",
    "\n",
    "# FLAG -- PERFORMANCE: The arc fitting approach used here could be sped up by explicitly\n",
    "#                      specifying a Jacobian function, see the markdown note above.\n",
    "\n",
    "# Compute coordinates from angle\n",
    "def circle(r, cx, cy, alpha):\n",
    "    x = r*np.cos(alpha) + cx\n",
    "    y = r*np.sin(alpha) + cy\n",
    "    return np.array([y,x])\n",
    "\n",
    "# Compute radius/radii given a center and a point/multiple points\n",
    "def radius(xc, yc, x, y):\n",
    "    return np.sqrt((x-xc)**2 + (y-yc)**2)\n",
    "\n",
    "# Loss: distance of data points from mean circle\n",
    "def circle_loss(c, x, y):\n",
    "    radii = radius(c[0], c[1], x, y)\n",
    "    return radii - radii.mean()\n",
    "\n",
    "# Subtraction of n1 and n2, wrapping around at minimum and maximum\n",
    "def wrap_sub(n1, n2, minimum=-np.pi, maximum=np.pi):\n",
    "    s = n1 - n2 \n",
    "    try:\n",
    "        s[s<=minimum] = maximum + (s[s<=minimum] - minimum)\n",
    "        s[s>=maximum] = minimum + (s[s>=maximum] - maximum)\n",
    "    except TypeError:\n",
    "        if s <= minimum: s = maximum + (s - minimum)\n",
    "        if s >= maximum: s = minimum + (s - maximum)\n",
    "    return s\n",
    "\n",
    "# Wrapped median of an array of angles\n",
    "# Note: The median of angles has to be computed on the unit vectors\n",
    "#       to avoid getting into trouble with wrapping!\n",
    "def wrap_median(angles, axis=None):\n",
    "    unitvecs = circle(1.0, 0.0, 0.0, angles)\n",
    "    median_vec = np.median(unitvecs, axis=axis+1 if axis is not None else None)\n",
    "    median_ang = np.arctan2(median_vec[0], median_vec[1])\n",
    "    return median_ang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Find incident vectors for each TN based on circular arc fitting\n",
    "\n",
    "# FLAG -- ROBUSTNESS: There is still an edge case in this where perfectly straight\n",
    "#                     lines are fit with a completely wrong (very small) circle.\n",
    "#                     Right now, this is handled as a \"silly exception\" for the\n",
    "#                     synthetic test sample, where the middle line between cells is\n",
    "#                     perfectly straight. The hope is that this will never occur\n",
    "#                     in real data - but if it does, the curent handling will\n",
    "#                     almost certainly fail, as it presupposes that the line is\n",
    "#                     not only perfectly straight but also perfectly aligned with\n",
    "#                     one of the image axes.\n",
    "# FLAG: SIMPLICITY -- This code seems a bit cumbersome. I think this may stem from the fact\n",
    "#                     that the code in general is written with a very 3D mindset whilst the\n",
    "#                     Brodland approach implemented here is still a 2D thing at this point!\n",
    "# FLAG: PERFORMANCE -- This takes a bit of time. There may be ways of speeding it up,\n",
    "#                      including just using fewer (but reliable) points to begin with.\n",
    "#                      Update: it's actually not too bad now.\n",
    "\n",
    "# Params\n",
    "close_points_radius  = 25.0 * res[-1]\n",
    "close_points_mindist = 10.0 * res[-1]\n",
    "close_points_minnum  = 20\n",
    "\n",
    "# Output\n",
    "TJs_vec_raw = {}\n",
    "\n",
    "# For each TJ...\n",
    "for TJ_ID in TJs_itsc_ev.keys():\n",
    "    \n",
    "    ## DEV-TEMP!\n",
    "    #if TJ_ID != (2,7,8):\n",
    "    #    continue\n",
    "    \n",
    "    # Find the IDs of the three connected interfaces\n",
    "    DJ_IDs = list(itertools.combinations(TJ_ID, 2))\n",
    "    \n",
    "    # Skip edge cases with more than 3\n",
    "    if len(DJ_IDs) > 3:\n",
    "        continue\n",
    "        \n",
    "    # Prepare an appropriate result array\n",
    "    TJs_vec_raw[TJ_ID] = np.empty((TJs_itsc_ev[TJ_ID].shape[0], 3, 3))  # Num. of TNs, 3 vectors, 3 dimensions\n",
    "    TJs_vec_raw[TJ_ID].fill(np.nan)\n",
    "    \n",
    "    # For each TN of the current TJ...\n",
    "    for TN_idx, TN in enumerate(TJs_itsc_ev[TJ_ID]): \n",
    "        \n",
    "        # For each connected interface...\n",
    "        for DJ_idx, DJ_ID in enumerate(DJ_IDs):\n",
    "            \n",
    "            # Skip if interface has been removed\n",
    "            if not DJ_ID in DJs:\n",
    "                continue\n",
    "            \n",
    "            # Get all the DJ points of the interface in the TN's z-plane\n",
    "            DNCs_z = DNCs[DJs[DJ_ID]][DNCs[DJs[DJ_ID]][:,0]==TN[0]]\n",
    "            \n",
    "            # If there are none, skip this TN\n",
    "            if DNCs_z.size == 0:\n",
    "                print(\"Skipped case at TJ_ID=\"+str(TJ_ID) + \", TN_idx=\" +str(TN_idx) + \n",
    "                      \", DJ_ID=\"+str(DJ_ID)+\" ->> lacks interface points!\")\n",
    "                continue\n",
    "                \n",
    "            # Get the DN points close to the TN\n",
    "            kdtree = spatial.cKDTree(DNCs_z)\n",
    "            KNNs   = kdtree.query_ball_point(TN, close_points_radius)\n",
    "            \n",
    "            # If there are none, skip this TN\n",
    "            if not KNNs:\n",
    "                print(\"Skipped case at TJ_ID=\"+str(TJ_ID) + \", TN_idx=\" +str(TN_idx) + \n",
    "                      \", DJ_ID=\"+str(DJ_ID)+\" ->> no close-by neighbors!\")\n",
    "                continue\n",
    "            \n",
    "            # Check there are very few, skip this TN\n",
    "            if len(KNNs) < close_points_minnum:\n",
    "                print(\"Skipped case at TJ_ID=\"+str(TJ_ID) + \", TN_idx=\" +str(TN_idx) + \n",
    "                      \", DJ_ID=\"+str(DJ_ID)+\" ->> not enough close-by neighbors!\")\n",
    "                continue\n",
    "            \n",
    "            # If they are all bunched up in a small space, skip this TN\n",
    "            furthest_NN_dist = np.sqrt(np.max(np.sum((DNCs_z[KNNs] - TN)**2.0, axis=1)))\n",
    "            if furthest_NN_dist < close_points_mindist:\n",
    "                print(\"Skipped case at TJ_ID=\"+str(TJ_ID) + \", TN_idx=\" +str(TN_idx) + \n",
    "                      \", DJ_ID=\"+str(DJ_ID)+\" ->> only a small cluster of neighbors!\")\n",
    "                continue\n",
    "            \n",
    "            # Prep data for fitting\n",
    "            x = np.concatenate([DNCs_z[KNNs,2], TN[2,np.newaxis]])\n",
    "            y = np.concatenate([DNCs_z[KNNs,1], TN[1,np.newaxis]])\n",
    "                \n",
    "            # Fit a circle to the data\n",
    "            center, ier = optimize.leastsq(circle_loss, [np.mean(x), np.mean(y)], args=(x, y))\n",
    "            cx, cy = center\n",
    "            r      = radius(cx, cy, x, y).mean()\n",
    "            \n",
    "            # Catch common fitting issue\n",
    "            # If the diameter of the fitted circle is smaller than the distance between\n",
    "            # TN and neighbor point furthest from it, the circle has likely been \"inscribed\"\n",
    "            # rather than arc-fitted.\n",
    "            is_inscribed = False\n",
    "            if 2*r < furthest_NN_dist:\n",
    "                is_inscribed = True\n",
    "                print(\"Detected inscribe circle for TJ_ID=\"+str(TJ_ID) + \", TN_idx=\" +str(TN_idx) + \n",
    "                      \", DJ_ID=\"+str(DJ_ID)+\" ->> used TN to cloud centroid as vectors!\")\n",
    "            \n",
    "            # Get angular position of the TN point\n",
    "            TN_alpha = np.arctan2(TN[1]-cy, TN[2]-cx)\n",
    "            \n",
    "            # Get correct sign for tangent vector direction\n",
    "            DNs_alpha = wrap_sub(np.arctan2(y-cy, x-cx), TN_alpha)\n",
    "            sign = np.sign(np.mean(DNs_alpha))\n",
    "\n",
    "            # Get tangent vector based on TN angle and small shift\n",
    "            TN_proj = circle(r, cx, cy, TN_alpha)\n",
    "            shifted = circle(r, cx, cy, TN_alpha+10e-5)\n",
    "            tangent = shifted - TN_proj\n",
    "            tangent = tangent * sign\n",
    "            \n",
    "            # Handle t\n",
    "            if is_inscribed:\n",
    "                tangent = np.array([np.mean(y)-TN[1], np.mean(x)-TN[2]])\n",
    "            \n",
    "            ## DEV-TEMP!\n",
    "            #plt.figure()\n",
    "            #plt.scatter(x, y)\n",
    "            #crc = circle(r, cx, cy, np.linspace(-np.pi, np.pi, 100))\n",
    "            #plt.plot(crc[1,:], crc[0,:])\n",
    "            #plt.scatter(TN[2], TN[1], c='r')\n",
    "            ##plt.xlim([50,60])\n",
    "            ##plt.ylim([70,80])\n",
    "            #plt.axis('equal')\n",
    "            #plt.title(TN[0])\n",
    "            #plt.show()\n",
    "            \n",
    "            # Normalize to magnitude 1\n",
    "            tangent = tangent / np.sqrt(np.sum(tangent**2.0))\n",
    "            \n",
    "            # Save the result\n",
    "            TJs_vec_raw[TJ_ID][TN_idx, DJ_idx, 0]  = 0.0\n",
    "            TJs_vec_raw[TJ_ID][TN_idx, DJ_idx, 1:] = tangent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Remove TNs with a nan\n",
    "\n",
    "# FLAG -- ROBUSTNESS: Removing nans here changes the number of TNs per TJ, which could cause\n",
    "#                     a mess when other TJ-associated data is needed again later. Hence, the\n",
    "#                     nan-afflicted TNs from all the relevant objects need to be removed,\n",
    "#                     which is a bit of a pain and could lead to unexpected behavior...\n",
    "#                     Update: Fortunately, the current implementation doesn't yield any NaNs,\n",
    "#                     at least not with the basic test sample.\n",
    "\n",
    "# For each TJ...\n",
    "for TJ_ID in list(TJs_vec_raw.keys()):\n",
    "    \n",
    "    # Find TNs with nans\n",
    "    nan_mask = np.any( np.isnan(TJs_vec_raw[TJ_ID]), axis=(-1, -2) )\n",
    "    \n",
    "    # Reassign without those TNs\n",
    "    TJs_vec_raw[TJ_ID] = TJs_vec_raw[TJ_ID][~nan_mask]\n",
    "    TJs_itsc_ev[TJ_ID] = TJs_itsc_ev[TJ_ID][~nan_mask]\n",
    "    TJs_itsc_t[TJ_ID] = TJs_itsc_t[TJ_ID][~nan_mask]\n",
    "    TJs_itsc_tangent[TJ_ID] = TJs_itsc_tangent[TJ_ID][~nan_mask]\n",
    "    \n",
    "    # Report\n",
    "    if nan_mask.sum() > 0:\n",
    "        print(\"Removed\", nan_mask.sum(), \"TNs due to NaNs among vectors;\", \n",
    "              (~nan_mask).sum(), \"remain in TJ\" + str(TJ_ID) + \".\")\n",
    "    \n",
    "    # Remove TJs that now have too few TNs or don't span enough z\n",
    "    if np.sum(~nan_mask) < min_TNs or np.unique(TJs_itsc_ev[TJ_ID][:,0]).size < min_z:\n",
    "        del TJs_vec_raw[TJ_ID]\n",
    "        del TJs_itsc_ev[TJ_ID]\n",
    "        del TJs_itsc_t[TJ_ID]\n",
    "        del TJs_itsc_tangent[TJ_ID]\n",
    "        print(\"Removed TJ\", TJ_ID, \"because it had less than the minimum number of TNs!\")\n",
    "\n",
    "# Report\n",
    "print(\"\\nNumber of TJs remaining:\", len(TJs_vec_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualize the resulting vectors\n",
    "\n",
    "@interact(z=(0, im.shape[0]-1, 1))\n",
    "def show_stack(z=im.shape[0]//2):\n",
    "    \n",
    "    # Prep and plot image\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.imshow(outlines_id[z], cmap='gray',\n",
    "               vmin=outlines_id.min(), vmax=outlines_id.max())\n",
    "    \n",
    "    # For each TJ...\n",
    "    for TJ_num, TJ_ID in enumerate(TJs_vec_raw.keys()):\n",
    "        \n",
    "        # DEV-TEMP!\n",
    "        #if TJ_ID != (0,2,3):\n",
    "        #    continue\n",
    "        \n",
    "        # Find indices of TNs in this plane (skip if none)\n",
    "        in_plane_indices = np.where( (TJs_itsc_ev[TJ_ID][:,0] / res[0] - 0.5)==z)[0]\n",
    "        if in_plane_indices.size == 0:\n",
    "            continue\n",
    "        \n",
    "        # For each such TN...\n",
    "        for TN_idx in in_plane_indices:\n",
    "            \n",
    "            # Get TE and vector coordinates\n",
    "            TN   = TJs_itsc_ev[TJ_ID][TN_idx]\n",
    "            vecs = TJs_vec_raw[TJ_ID][TN_idx]\n",
    "        \n",
    "            # Convert back to image space\n",
    "            TN = (TN / res) - 0.5\n",
    "            vecs = (vecs / res)\n",
    "        \n",
    "            # Plot all the vectors\n",
    "            for vec in vecs:\n",
    "                plt.plot([TN[2], TN[2]+vec[2]*10], \n",
    "                         [TN[1], TN[1]+vec[1]*10],\n",
    "                         'r-', lw=2)\n",
    "        \n",
    "    # Finish\n",
    "    #plt.xlim([100,150])\n",
    "    #plt.ylim([110,160])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Show the resulting vector triplets \n",
    "\n",
    "# For each TJ...\n",
    "cols = ['r','g','b']\n",
    "for TJ_ID in TJs_vec_raw.keys():\n",
    "    \n",
    "    # Prep\n",
    "    plt.figure(figsize=(4,4))\n",
    "    \n",
    "    # Plot each vec...\n",
    "    for vec in TJs_vec_raw[TJ_ID]:\n",
    "        for i,v in enumerate(vec):\n",
    "            plt.plot([0,v[2]], [0,v[1]], c=cols[i])\n",
    "            \n",
    "    # Finalize\n",
    "    plt.title(str(TJ_ID))\n",
    "    plt.xlabel('x'); plt.ylabel('y')\n",
    "    plt.xlim([-1.1, 1.1])\n",
    "    plt.ylim([-1.1, 1.1])\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projecting Incident Vectors onto the TJ-Orthogonal Plane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sympy function to project close-by outline points onto a TN's TJ-orthogonal plane\n",
    "\n",
    "# FLAG: PERFORMANCE -- Save the resulting numpy func so that the symbolic solving doesn't need \n",
    "#                      to be rerun each time the code is executed! This is probably best done\n",
    "#                      by copying the function out into a .py file and importing it from there.\n",
    "#                      In the process, axis keywords could perhaps be added to handle vectorized\n",
    "#                      execution across many points/planes (see issue flag below).\n",
    "# FLAG: ROBUSTNESS -- Simply doing Gram-Schmidt as we currently do does not preserve the\n",
    "#                     uv-coordinate system within the plane across multiple TNs of a TJ. \n",
    "#                     Under certain circumstances (when values of the normal vector cross\n",
    "#                     zero), this can even lead to sudden 'flipping' of the orientation of\n",
    "#                     the plane. Currently, this is implicitly being \"fixed\" downstream \n",
    "#                     since the vector triplets are being aligned by rotation and flipping\n",
    "#                     prior to their reduction to a consensus triplet. However, it might be\n",
    "#                     more clean and robust to do something slightly more sophisticated than\n",
    "#                     classical Gram-Schmidt in order to enforce consistency.\n",
    "\n",
    "# Import sympy symbols\n",
    "from sympy.abc import q,r,s,  x,y,z  # (normal vector), (point to be projected)\n",
    "\n",
    "# Use Gram-Schmidt orthogonalization to create orthonormal vectors defining the in-plane\n",
    "# coordinate system given three arbitrary vectors, the first of which is the normal vector\n",
    "# of the plane. The other two (defining the in-plane directionalities) are arbitrarily \n",
    "# chosen such that they will never fall onto the normal vector or onto each other.\n",
    "orthonormals = sym.GramSchmidt([sym.Matrix([q,         r,         s]),  # Normal vec to plane -> first coordinate vec\n",
    "                                sym.Matrix([q, 2*(r+0.1), 3*(s+0.1)]),  # Arbitrary vec not on the normal vec\n",
    "                                sym.Matrix([2*(q+0.1), 3*(r+0.1), s])], # Arbitrary vec not on either other vec\n",
    "                                orthonormal=True)           # Normalize resulting orthogonal vectors\n",
    "\n",
    "# With the resulting orthonormals defining the new coordinate system, the projection\n",
    "# of points into it is just a straightforward dot product.\n",
    "projection_pt = sym.Matrix([x, y, z])\n",
    "proj_d = orthonormals[0].dot(projection_pt)  # Distance from plane\n",
    "proj_u = orthonormals[1].dot(projection_pt)  # Coordinate along first axis in plane\n",
    "proj_v = orthonormals[2].dot(projection_pt)  # Coordinate along second axis in plane\n",
    "\n",
    "# Lambdify\n",
    "lambda_dist = sym.utilities.lambdify((q,r,s,x,y,z), proj_d, modules='numpy')\n",
    "lambda_u    = sym.utilities.lambdify((q,r,s,x,y,z), proj_u, modules='numpy')\n",
    "lambda_v    = sym.utilities.lambdify((q,r,s,x,y,z), proj_v, modules='numpy')\n",
    "\n",
    "# Wrap into a function (sequential)\n",
    "def p2p_projection(normal_vec, pt_coords):\n",
    "        \n",
    "    # Unpack inputs\n",
    "    q,r,s = normal_vec[2], normal_vec[1], normal_vec[0]\n",
    "    x,y,z = pt_coords[:,2], pt_coords[:,1], pt_coords[:,0]\n",
    "    \n",
    "    # Run projection\n",
    "    dists = np.abs(lambda_dist(q,r,s,x,y,z))\n",
    "    p_u   = lambda_u(q,r,s,x,y,z)\n",
    "    p_v   = lambda_v(q,r,s,x,y,z)\n",
    "    \n",
    "    # Pack and return outputs\n",
    "    projected = np.array([p_u, p_v]).T\n",
    "    return projected, dists\n",
    "\n",
    "## Wrap into a function (vectorized)\n",
    "## FLAG -- ISSUE: This does not work as intended! It runs but does not yield the same results\n",
    "##                as the sequential version. There is likely an missing `axis=` kwarg in one \n",
    "##                of the numpy functions substituted by lambdify. This could perhaps be fixed\n",
    "##                by manual inspection of the projection function.\n",
    "#def p2p_projection_vectorized(normal_vec, pt_coords):\n",
    "#    \n",
    "#    # Unpack inputs\n",
    "#    q,r,s = normal_vec[..., 2, np.newaxis], normal_vec[..., 1, np.newaxis], normal_vec[..., 0, np.newaxis]\n",
    "#    x,y,z = pt_coords[..., 2], pt_coords[..., 1], pt_coords[..., 0]\n",
    "#    \n",
    "#    # Run projection\n",
    "#    dists = np.abs(lambda_dist(q,r,s,x,y,z))\n",
    "#    p_u   = lambda_u(q,r,s,x,y,z)\n",
    "#    p_v   = lambda_v(q,r,s,x,y,z)\n",
    "#    \n",
    "#    # Pack and return outputs\n",
    "#    projected = np.rollaxis(np.array([p_u, p_v]), 2)\n",
    "#    projected = np.rollaxis(projected, 2)\n",
    "#    return projected, dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Project vectors onto the TJ-orthogonal plane\n",
    "\n",
    "# For each TJ...\n",
    "TJs_vec_proj = {}\n",
    "for TJ_ID in TJs_vec_raw.keys():\n",
    "    \n",
    "    # Get vectors\n",
    "    proj_vecs = TJs_vec_raw[TJ_ID]\n",
    "    \n",
    "    # Get TJ-tangent vectors (normal to TJ-orthogonal planes)\n",
    "    proj_tangents = TJs_itsc_tangent[TJ_ID]\n",
    "    \n",
    "    # Project vectors onto normal plane (sequential)\n",
    "    projs = []\n",
    "    for vec, norm in zip(proj_vecs, proj_tangents):\n",
    "        proj, _ = p2p_projection(norm, vec)  # Project\n",
    "        proj = (proj.T / np.sqrt(np.sum(proj**2.0, axis=1))).T  # Renormalize\n",
    "        projs.append(proj)  # Keep\n",
    "    projs = np.array(projs)  # Convert\n",
    "    \n",
    "    # Keep results\n",
    "    TJs_vec_proj[TJ_ID] = projs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Show the resulting projected vector triplets \n",
    "\n",
    "# FLAT: NOTE -- These are no longer aligned, which is okay but not ideal;\n",
    "#               see ROBUSTNESS flag in sympy code above.\n",
    "\n",
    "# For each TJ...\n",
    "cols = ['r','g','b']\n",
    "for TJ_ID in TJs_vec_proj.keys():\n",
    "    \n",
    "    # Prep\n",
    "    plt.figure(figsize=(4,4))\n",
    "    \n",
    "    # Plot each vec...\n",
    "    for vec in TJs_vec_proj[TJ_ID]:\n",
    "        for i,v in enumerate(vec):\n",
    "            plt.plot([0,v[1]], [0,v[0]], c=cols[i])\n",
    "            \n",
    "    # Finalize\n",
    "    plt.title(str(TJ_ID))\n",
    "    plt.xlabel('v'); plt.ylabel('u')\n",
    "    plt.xlim([-1.1, 1.1])\n",
    "    plt.ylim([-1.1, 1.1])\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aligning Incident Vectors Along TJs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Align triplets based on first vector & flip those that are the wrong way around\n",
    "\n",
    "# For each TJ...\n",
    "TJs_vec_aligned = {}\n",
    "for TJ_ID in TJs_vec_proj.keys():\n",
    "    \n",
    "    # Prep output container\n",
    "    triplets_aligned = np.empty_like(TJs_vec_proj[TJ_ID])\n",
    "    \n",
    "    ## Rotate each triplet to lay the first vector onto zero angle [sequential]\n",
    "    #angles_zeroed = np.empty((TJs_vec_proj[TJ_ID].shape[0], 3))\n",
    "    #for t,triplet in enumerate(TJs_vec_proj[TJ_ID]):\n",
    "    #    angles_raw = np.arctan2(triplet[:,0], triplet[:,1])\n",
    "    #    angles_zeroed[t] = angles_raw - angles_raw[0]\n",
    "     \n",
    "    # Rotate each triplet to lay the first vector onto zero angle [vectorized]\n",
    "    angles_raw = np.arctan2(TJs_vec_proj[TJ_ID][:,:,0], TJs_vec_proj[TJ_ID][:,:,1])\n",
    "    angles_zeroed = wrap_sub(angles_raw, angles_raw[:, 0, np.newaxis])\n",
    "\n",
    "    ## Function: if flipped is better than the consensus (here the median), then flip [sequential]\n",
    "    #def flip_improvement(angles_zeroed):\n",
    "    #    median_ang = wrap_median(angles_zeroed, axis=0)\n",
    "    #    for t in range(len(angles_zeroed)):\n",
    "    #        diff_original = np.abs(wrap_sub( angles_zeroed[t], median_ang)) \n",
    "    #        diff_flipped  = np.abs(wrap_sub(-angles_zeroed[t], median_ang))\n",
    "    #        if np.sum(diff_flipped) < np.sum(diff_original):\n",
    "    #            angles_zeroed[t] = - angles_zeroed[t]\n",
    "    #    return angles_zeroed\n",
    "    \n",
    "    # Function: if flipped is better than the consensus (here the median), then flip [vectorized]\n",
    "\n",
    "    def flip_improvement(angles_zeroed):\n",
    "        median_ang = wrap_median(angles_zeroed, axis=0)\n",
    "        diff_original = np.abs(wrap_sub( angles_zeroed, median_ang))\n",
    "        diff_flipped  = np.abs(wrap_sub(-angles_zeroed, median_ang))\n",
    "        flip_mask = np.sum(diff_flipped, axis=1) < np.sum(diff_original, axis=1)\n",
    "        angles_zeroed[flip_mask] = -angles_zeroed[flip_mask]\n",
    "        return angles_zeroed\n",
    "    \n",
    "    # Run flip improvement until there is either...\n",
    "    # ...no change from one step to the next, or\n",
    "    # ...no improvement since 5 steps ago\n",
    "    median  = wrap_median(angles_zeroed, axis=0)\n",
    "    losses  = [np.abs(wrap_sub(angles_zeroed, median))]\n",
    "    counter = 0\n",
    "    while True:\n",
    "        \n",
    "        # Run a flip\n",
    "        angles_zeroed_new = flip_improvement(angles_zeroed)\n",
    "        \n",
    "        # Break if it changed nothing\n",
    "        if np.all(angles_zeroed==angles_zeroed_new):\n",
    "            break\n",
    "            \n",
    "        # Otherwise, compute and keep the new loss\n",
    "        median = wrap_median(angles_zeroed_new, axis=0)\n",
    "        losses.append(np.abs(wrap_sub(angles_zeroed_new, median)))\n",
    "        \n",
    "        # Break if the new loss is worse or equal to the loss 5 steps ago\n",
    "        if (counter >= 5) and losses[-1] >= losses[-6]:\n",
    "            break\n",
    "            \n",
    "        # Update\n",
    "        angles_zeroed = angles_zeroed_new\n",
    "        counter += 1\n",
    "    \n",
    "    # Convert back to unit vectors\n",
    "    for t in range(len(angles_zeroed)):\n",
    "        triplets_aligned[t] = circle(1.0, 0.0, 0.0, angles_zeroed[t]).T\n",
    "        \n",
    "    # Store results\n",
    "    TJs_vec_aligned[TJ_ID] = triplets_aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Show the resulting aligned vector triplets \n",
    "\n",
    "# For each TJ...\n",
    "cols = ['r','g','b']\n",
    "for TJ_ID in TJs_vec_aligned.keys():\n",
    "    \n",
    "    # Prep\n",
    "    plt.figure(figsize=(4,4))\n",
    "    \n",
    "    # Plot each vec...\n",
    "    for vec in TJs_vec_aligned[TJ_ID]:\n",
    "        for i,v in enumerate(vec):\n",
    "            plt.plot([0,v[1]], [0,v[0]], c=cols[i])\n",
    "            \n",
    "    # Finalize\n",
    "    plt.title(str(TJ_ID))\n",
    "    plt.xlabel('v'); plt.ylabel('u')\n",
    "    plt.xlim([-1.1, 1.1])\n",
    "    plt.ylim([-1.1, 1.1])\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Consensus Incident Vector Triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate a consensus incident vector triplet for each TJ\n",
    "\n",
    "# FLAG -- PRECISION, FLAG -- ROBUSTNESS: This is currently done in a very simple fashion.\n",
    "#                                        It probably works fine / doesn't matter much for\n",
    "#                                        data with a high z-resolution. However, there may\n",
    "#                                        be room for improvement for low z-resolution data!\n",
    "\n",
    "# For each TJ...\n",
    "TJs_vec_consensus = {}\n",
    "for TJ_ID in TJs_vec_aligned.keys():\n",
    "    \n",
    "    ## Compute the mean of vectors within the 25-75th percentile [OLD VERSION]\n",
    "    ## Note: Whilst this seems more sophisticated than just using the median, the percentile \n",
    "    ##       masking as done here does not account for vector circle wrapping. Also, this \n",
    "    ##       implementation can have weird cases where for some vectors one of their dimensions \n",
    "    ##       is counted toward the consensus but not the other.\n",
    "    #vecs_cons = np.empty((3,2))\n",
    "    #loss_cons = np.empty(3)\n",
    "    #p25, p75 = np.percentile(TJs_vec_aligned[TJ_ID], [25, 75], axis=0)\n",
    "    #for v in range(3):\n",
    "    #    vecs = TJs_vec_aligned[TJ_ID][:,v,:]\n",
    "    #    mask = (vecs >= p25[v]) & (vecs <= p75[v])\n",
    "    #    mean_v = np.mean(vecs[mask[:,0],0])\n",
    "    #    mean_u = np.mean(vecs[mask[:,1],1])\n",
    "    #    vecs_cons[v] = [mean_v, mean_u]\n",
    "    \n",
    "    # Compute the median of vectors [CURRENT VERSION]\n",
    "    angs = np.arctan2(TJs_vec_aligned[TJ_ID][:,:,0], TJs_vec_aligned[TJ_ID][:,:,1])\n",
    "    angs_consensus = wrap_median(angs, axis=0)\n",
    "    vecs_consensus = circle(1.0, 0.0, 0.0, angs_consensus).T\n",
    "    \n",
    "    # Compute the 'loss' of the consensus & drop poor TJs\n",
    "    # Note: The 'loss' is the mean deviation from the consensus. The threshold is chosen\n",
    "    #       such that this mean deviation must not be greater than thresh_cons% of the\n",
    "    #       unit circle.\n",
    "    losses = np.sum(np.abs(wrap_sub(angs, angs_consensus)), axis=0) / angs.shape[0]\n",
    "    if np.any(losses > 2.0*np.pi*(t_cons/100)):\n",
    "        print(\"Removed TJ due to low consistency. TJ_ID:\", TJ_ID)\n",
    "        continue\n",
    "    \n",
    "    # YAH!\n",
    "    # - Measure the \"loss\" of the resulting consensus and find an appropriate way of discarding 'bad' cases\n",
    "    # + Consider re-including TNs that were excluded as 'stubs' in the arc fitting; they are now covered by the centroid fix\n",
    "    # ~ Add in the excluding-DJs-if-no-information-in-G thingy!\n",
    "    # ~ Port to VAL_Improved, as we may reach Brodland-like performance there (and that's good enough)\n",
    "    # ~ Don't forget to see if you can have a go at the simulated data (which may be more clean...)\n",
    "        \n",
    "    # Renormalize\n",
    "    TJs_vec_consensus[TJ_ID] = (vecs_consensus.T / np.sqrt(np.sum(vecs_consensus**2.0, axis=1))).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Show the resulting consensus vector triplets \n",
    "\n",
    "# For each TJ...\n",
    "cols = ['r','g','b']\n",
    "for TJ_ID in TJs_vec_consensus.keys():\n",
    "    \n",
    "    # Prep\n",
    "    plt.figure(figsize=(4,4))\n",
    "    \n",
    "    # Plot individual vecs...\n",
    "    for vec in TJs_vec_aligned[TJ_ID]:\n",
    "        for i,v in enumerate(vec):\n",
    "            plt.plot([0,v[1]], [0,v[0]], c=cols[i], alpha=0.3)\n",
    "            \n",
    "    # Plot consensus vecs\n",
    "    for i,v in enumerate(TJs_vec_consensus[TJ_ID]):\n",
    "        plt.plot([0,v[1]], [0,v[0]], c=cols[i], lw=4)\n",
    "            \n",
    "    # Finalize\n",
    "    plt.title(str(TJ_ID))\n",
    "    plt.xlabel('v'); plt.ylabel('u')\n",
    "    plt.xlim([-1.1, 1.1])\n",
    "    plt.ylim([-1.1, 1.1])\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving the Force Balance Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prep: Assembling Equation Matrix G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Assemble G\n",
    "\n",
    "# Initialize zero matrix of shape (2 * num of TJs, num of DJs)\n",
    "G = np.zeros((2*len(TJs_vec_consensus), len(DJs)))\n",
    "\n",
    "# For each TJ...\n",
    "DJs_all_IDs = list(DJs.keys())\n",
    "for TJ_idx, TJ_ID in enumerate(TJs_vec_consensus.keys()):\n",
    "    \n",
    "    # Get all relevant DJs\n",
    "    DJ_IDs  = list(itertools.combinations(TJ_ID, 2))\n",
    "    \n",
    "    # For each DJ...\n",
    "    for DJ_ref, DJ_ID in enumerate(DJ_IDs):\n",
    "        \n",
    "        # Skip if interface has been removed\n",
    "        if not DJ_ID in DJs:\n",
    "            continue\n",
    "        \n",
    "        # Get index (in G) of the current DJ\n",
    "        DJ_idx = DJs_all_IDs.index(DJ_ID)\n",
    "\n",
    "        # Fill the appropriate positions in G\n",
    "        G[TJ_idx, DJ_idx] = TJs_vec_consensus[TJ_ID][DJ_ref][0]\n",
    "        G[len(TJs_vec_consensus)+TJ_idx, DJ_idx] = TJs_vec_consensus[TJ_ID][DJ_ref][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualize the result\n",
    "print(G.shape) # Should show G.shape[0] >= G.shape[1] (num of eqs >= num of interfaces)!\n",
    "plt.imshow(G)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Remove any DNs in G that do not have equations\n",
    "\n",
    "# Find empty columns\n",
    "mask = np.ones((G.shape[1]), dtype=np.bool)\n",
    "for i in range(G.shape[1]):\n",
    "    if np.sum(G[:,i]) == 0.0:\n",
    "        print(\"Found empty column in G with index\", i, \"and DJ_ID\", str(DJs_all_IDs[i])+\"!\")\n",
    "        mask[i] = 0\n",
    "        \n",
    "# Remove empty columns\n",
    "G = G[:,mask]\n",
    "\n",
    "# Remove corresponding DJs from relevant linked objects\n",
    "DJs_all_IDs = [DJs_all_IDs[i] for i in range(len(DJs_all_IDs)) if mask[i]]\n",
    "for DJ_ID in list(DJs.keys()):\n",
    "    if DJ_ID not in DJs_all_IDs: \n",
    "        del DJs[DJ_ID]\n",
    "\n",
    "# Report\n",
    "print(G.shape)\n",
    "print(len(DJs_all_IDs))\n",
    "print(len(DJs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solve using the `contraints` kwarg of scipy's minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define loss and constraints\n",
    "\n",
    "# Loss: sum of square deviations of equilibrium equations\n",
    "def eq_loss_c(gammas, G):\n",
    "    loss = (np.dot(G, gammas))**2.0\n",
    "    return np.sum(loss)\n",
    "\n",
    "# Constraint: the mean of tensions must be 1\n",
    "def eq_constraint(gammas):\n",
    "    c = np.mean(gammas) - 1\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run the fit\n",
    "fit = optimize.minimize(eq_loss_c, np.ones(G.shape[1]), args=(G,), \n",
    "                        constraints={'type':'eq', 'fun':eq_constraint})\n",
    "tensions_c = fit.x\n",
    "print(tensions_c)\n",
    "\n",
    "# FLAG -- ISSUE: The softest interface has a negative tension. I'm not sure\n",
    "#                if that indicates a problem. It might be perfectly fine;\n",
    "#                after all, the tensions are relative to the mean and they\n",
    "#                are effective surface tensions, so high adhesion should\n",
    "#                be able to make them net-negative. Furthermore, this\n",
    "#                synthetic test sample has been constructed from geometric\n",
    "#                objects, so it doesn't represent a realistic structure.\n",
    "\n",
    "# Keep results\n",
    "DJs_tensions_c = {DJ_ID:tensions_c[DJ_num] for DJ_num, DJ_ID in enumerate(DJs_all_IDs)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Show tensions on image stack\n",
    "\n",
    "@interact(z=(0, im.shape[0]-1, 1))\n",
    "def show_stack(z=im.shape[0]//2):\n",
    "    \n",
    "    # Prep and plot image\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.imshow(outlines_id[z], cmap='gray',\n",
    "               vmin=outlines_id.min(), vmax=outlines_id.max())\n",
    "    \n",
    "    # For each DJ...\n",
    "    for DJ_num, DJ_ID in enumerate(DJs.keys()):\n",
    "        \n",
    "        # Get the DJ's DNs in the selected z plane\n",
    "        DNs_in_plane = DNIs[DJs[DJ_ID]][DNIs[DJs[DJ_ID]][:,0]==z]\n",
    "        \n",
    "        # Plot the points\n",
    "        plt.scatter(DNs_in_plane[:, 2], DNs_in_plane[:, 1],\n",
    "                    c=[tensions_c[DJ_num] for _ in range(DNs_in_plane.shape[0])],\n",
    "                    vmin=np.min(tensions_c), vmax=np.max(tensions_c),\n",
    "                    cmap='viridis', s=20)\n",
    "    \n",
    "    # Finish\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solve using Brodland et alii's Lagrange Multiplier Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prepare the matrix\n",
    "\n",
    "# Gdot\n",
    "Gdot = np.dot(G.T, G)\n",
    "\n",
    "# Show\n",
    "plt.imshow(Gdot)\n",
    "plt.show()\n",
    "\n",
    "# Add the constraints\n",
    "Gready = np.zeros((Gdot.shape[0]+1, Gdot.shape[1]+1))\n",
    "Gready[:Gdot.shape[0], :Gdot.shape[1]] = Gdot\n",
    "Gready[-1,:-1] = 1.0\n",
    "Gready[:-1,-1] = 1.0\n",
    "\n",
    "# Show\n",
    "plt.imshow(Gready)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define loss\n",
    "\n",
    "# Loss combining tension fit and constraint\n",
    "def eq_loss_l(gammas_lagrange, Gready):\n",
    "    loss = np.sum(np.dot(Gready[:-1], gammas_lagrange)**2.0)  # Fit loss\n",
    "    loss += (np.dot(Gready[-1], gammas_lagrange) - (gammas_lagrange.size-1))**2.0  # Constraint loss    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run the fit\n",
    "fit = optimize.minimize(eq_loss_l, np.ones(len(DJs)+1), args=(Gready,))\n",
    "tensions_l = fit.x[:-1]\n",
    "lagrange = fit.x[-1]\n",
    "print(tensions_l)\n",
    "print(lagrange)\n",
    "\n",
    "# FLAG -- ISSUE: Negative tension value, same as above with the scipy-based\n",
    "#                approach. See flag there for more info.\n",
    "\n",
    "# Keep results\n",
    "DJs_tensions_l = {DJ_ID:tensions_l[DJ_num] for DJ_num, DJ_ID in enumerate(DJs_all_IDs)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Show tensions on image stack\n",
    "\n",
    "@interact(z=(0, im.shape[0]-1, 1))\n",
    "def show_stack(z=im.shape[0]//2):\n",
    "    \n",
    "    # Prep and plot image\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.imshow(outlines_id[z], cmap='gray',\n",
    "               vmin=outlines_id.min(), vmax=outlines_id.max())\n",
    "    \n",
    "    # For each DJ...\n",
    "    for DJ_num, DJ_ID in enumerate(DJs.keys()):\n",
    "        \n",
    "        # Get the DJ's DNs in the selected z plane\n",
    "        DNs_in_plane = DNIs[DJs[DJ_ID]][DNIs[DJs[DJ_ID]][:,0]==z]\n",
    "        \n",
    "        # Plot the points\n",
    "        plt.scatter(DNs_in_plane[:, 2], DNs_in_plane[:, 1],\n",
    "                    c=[tensions_l[DJ_num] for _ in range(DNs_in_plane.shape[0])],\n",
    "                    vmin=np.min(tensions_l), vmax=np.max(tensions_l),\n",
    "                    cmap='viridis', s=20, alpha=0.5)\n",
    "    \n",
    "    # Finish\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare Solver vs Lagrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot against each other\n",
    "\n",
    "# Prep\n",
    "plt.figure(figsize=(5,5))\n",
    "\n",
    "# Plot\n",
    "plt.scatter(tensions_l, tensions_c, s=50,\n",
    "            c='darkblue', lw=0.5, edgecolor='cyan')\n",
    "\n",
    "# Add equality line\n",
    "xlims, ylims = plt.gca().get_xlim(), plt.gca().get_ylim()\n",
    "plt.plot([-10,10], [-10,10], 'k-', zorder=-1, lw=1, alpha=0.5)\n",
    "plt.xlim(xlims); plt.ylim(ylims)\n",
    "\n",
    "# Labels\n",
    "plt.xlabel(\"inferred tension\\n[lagrange multiplier]\")\n",
    "plt.ylabel(\"inferred tension\\n[scipy constraint]\")\n",
    "\n",
    "# Finalize\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with CellFIT3D and Microaspiration Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load validation data\n",
    "\n",
    "# Get Pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Read\n",
    "df_t4 = pd.read_csv(r'..\\Data\\Brodland\\Brodland_20190726_clean\\t000004\\t000004_OUT.txt', delimiter='\\t')\n",
    "df_t9 = pd.read_csv(r'..\\Data\\Brodland\\Brodland_20190726_clean\\t000009\\t000009_OUT.txt', delimiter='\\t')\n",
    "\n",
    "# Add color information\n",
    "colors = {'a':'darkorange', 'b':'lightgray', 'c':'orange', \n",
    "          'd':'blue', 'e':'green', 'f':'darkblue', \n",
    "          'g':'brown', 'h':'gray'}\n",
    "df_t4['colors'] = df_t4['Interface_abc'].map(colors)\n",
    "df_t9['colors'] = df_t9['Interface_abc'].map(colors)\n",
    "\n",
    "# Remove rows with NaNs\n",
    "df_t4 = df_t4.dropna(axis=0)\n",
    "df_t9 = df_t9.dropna(axis=0)\n",
    "\n",
    "# Report\n",
    "df_t4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add the pyCFI results to the dataframe\n",
    "\n",
    "# First fix the interface tuples in the df (all non-zero values are off by one...)\n",
    "for i, tup in enumerate(df_t4['Interface_tuple']):\n",
    "    tup0 = int(tup.split(',')[0][1:])\n",
    "    tup1 = int(tup.split(',')[1][:-1])\n",
    "    if tup1 > 0:\n",
    "        tup1 -= 1\n",
    "    df_t4['Interface_tuple'].iat[i] = str((tup0, tup1))\n",
    "    \n",
    "# Create a small df of the tension data\n",
    "df_tensions_c = pd.DataFrame(list(DJs_tensions_c.items()), columns=['Interface_tuple', 'pyCFI_c'])\n",
    "df_tensions_l = pd.DataFrame(list(DJs_tensions_c.items()), columns=['Interface_tuple', 'pyCFI_l'])\n",
    "df_tensions = pd.merge(df_tensions_c, df_tensions_l, on='Interface_tuple')\n",
    "df_tensions['Interface_tuple'] = df_tensions['Interface_tuple'].astype(str)\n",
    "\n",
    "# Add the tension data to the validation data\n",
    "df_t4 = pd.merge(df_t4, df_tensions, on='Interface_tuple')\n",
    "\n",
    "# Add rescaled tension data\n",
    "df_t4['pyCFI_c_rescaled'] = df_t4['pyCFI_c'] / df_t4['pyCFI_c'].mean() * df_t4['Pipette'].mean()\n",
    "df_t4['pyCFI_l_rescaled'] = df_t4['pyCFI_l'] / df_t4['pyCFI_l'].mean() * df_t4['Pipette'].mean()\n",
    "\n",
    "# Report\n",
    "df_t4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Show result\n",
    "\n",
    "# Some stats\n",
    "from scipy.stats import pearsonr\n",
    "print('pyCFI:  ', pearsonr(df_t4['Pipette'], df_t4['pyCFI_l_rescaled']))\n",
    "print('CellFIT:', pearsonr(df_t4['Pipette'], df_t4['CellFIT_rescaled']))\n",
    "\n",
    "# Plot data\n",
    "plt.scatter(df_t4['Pipette'], df_t4['pyCFI_l_rescaled'], label='pyCFI')\n",
    "plt.scatter(df_t4['Pipette'], df_t4['CellFIT_rescaled'], label='CellFIT')\n",
    "\n",
    "# Legend\n",
    "plt.legend()\n",
    "\n",
    "# Perfect correlation line\n",
    "plt.plot([0, 1000], [0, 1000], 'k-', \n",
    "         lw=1.0, zorder=1)\n",
    "\n",
    "# Adjust axis limits\n",
    "plt.xlim([170, 290])\n",
    "plt.ylim([100, 350])\n",
    "\n",
    "# Show\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3base]",
   "language": "python",
   "name": "conda-env-py3base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
